"""
å†™ä½œæ™ºèƒ½ä½“
æ‰§è¡Œç« èŠ‚å†™ä½œä»»åŠ¡ï¼Œç”Ÿæˆå®Œæ•´çš„ä¸€çº§æ ‡é¢˜ç« èŠ‚ï¼ˆåŒ…å«è¯¥ä¸€çº§æ ‡é¢˜ä¸‹çš„æ‰€æœ‰äºŒçº§æ ‡é¢˜ï¼‰
"""
import os
from typing import List, Dict, Any, Optional

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain_community.chat_models import ChatOpenAI

from ..tools.writing_history import WritingHistoryManager


class WritingAgentError(Exception):
    """å†™ä½œæ™ºèƒ½ä½“é”™è¯¯"""
    pass


class WritingAgent:
    """å†™ä½œæ™ºèƒ½ä½“ - æ‰§è¡Œç« èŠ‚å†™ä½œä»»åŠ¡"""
    
    def __init__(self, history_manager: Optional[WritingHistoryManager] = None):
        """
        åˆå§‹åŒ–å†™ä½œæ™ºèƒ½ä½“
        
        Args:
            history_manager: å†å²å†™ä½œç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        """
        # ç¡®ä¿åŠ è½½ .env æ–‡ä»¶
        from dotenv import load_dotenv
        from pathlib import Path
        env_path = Path(__file__).parent.parent.parent.parent / ".env"
        load_dotenv(dotenv_path=env_path, override=False)
        
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise WritingAgentError("DASHSCOPE_API_KEY æœªé…ç½®")
        
        self.llm = ChatOpenAI(
            model="qwen-plus",
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            temperature=0.8,
        )
        self.history_manager = history_manager
    
    def write_section(
        self,
        section: Dict[str, Any],
        search_results: List[Dict[str, Any]],
        history_sections: List[str],
        outline: str,
        previous_sections_summary: Optional[str] = None,
        total_words: Optional[int] = None,
        total_sections: Optional[int] = None,
        written_words: Optional[int] = None,
        enable_chart: bool = True
    ) -> Dict[str, Any]:
        """
        æ’°å†™å®Œæ•´ç« èŠ‚ï¼ˆæŒ‰ä¸€çº§æ ‡é¢˜ä¸ºå•ä½ï¼‰
        
        Args:
            section: ç« èŠ‚ä¿¡æ¯ï¼ŒåŒ…å« section_id, level1_title, level2_titles, index
            search_results: æ£€ç´¢ç»“æœåˆ—è¡¨
            history_sections: å†å²ç« èŠ‚å†…å®¹åˆ—è¡¨
            outline: å®Œæ•´å¤§çº²
            previous_sections_summary: å‰æ–‡æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            total_words: æŠ¥å‘Šæ€»å­—æ•°ï¼ˆå¯é€‰ï¼‰
            total_sections: æ€»ç« èŠ‚æ•°ï¼ˆå¯é€‰ï¼‰
            written_words: å·²å†™å†…å®¹çš„å­—æ•°ï¼ˆå¯é€‰ï¼‰
            enable_chart: æ˜¯å¦å¯ç”¨å›¾è¡¨ç”Ÿæˆï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
        
        Returns:
            å­—å…¸ï¼ŒåŒ…å«ç« èŠ‚å†…å®¹ã€å¼•ç”¨ä¿¡æ¯å’Œå›¾è¡¨éœ€æ±‚ï¼š
            {
                "content": str,  # Markdownæ ¼å¼çš„ç« èŠ‚å†…å®¹
                "citations": List[Dict],  # å®é™…ä½¿ç”¨çš„å‚è€ƒèµ„æ–™åˆ—è¡¨
                "chart_requirement": Optional[Dict]  # å›¾è¡¨éœ€æ±‚ï¼ˆå¦‚æœéœ€è¦å›¾è¡¨ï¼‰
            }
        """
        level1_title = section.get("level1_title", "")
        level2_titles = section.get("level2_titles", [])
        section_index = section.get("index", 0)
        
        if not level1_title:
            raise WritingAgentError("ä¸€çº§æ ‡é¢˜ä¸èƒ½ä¸ºç©º")
        # å…è®¸äºŒçº§æ ‡é¢˜ä¸ºç©ºï¼Œè¿™æ ·å¯ä»¥ç›´æ¥å†™ä¸€çº§æ ‡é¢˜å†…å®¹è€Œä¸éœ€è¦ç»†åˆ†
        
        # ç»™æ¯ä¸ªæ£€ç´¢ç»“æœåˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ ref_id
        for idx, result in enumerate(search_results, 1):
            result['ref_id'] = f"ref_{idx}"
        
        # æ„å»ºå­—æ•°è¦æ±‚è¯´æ˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šåªå‘ŠçŸ¥æ€»å­—æ•°å’Œå½“å‰è¿›åº¦ï¼Œè®©æ¨¡å‹è‡ªä¸»è§„åˆ’ï¼‰
        if total_words:
            words_instruction = f"""3. **å­—æ•°è¦æ±‚**ï¼š
   - æ•´ç¯‡æŠ¥å‘Šæ€»å­—æ•°çº¦ï¼š{total_words}å­—
   - å½“å‰å·²å†™å­—æ•°çº¦ï¼š{written_words or 0}å­—
   - å…±{total_sections}ä¸ªç« èŠ‚ï¼Œå½“å‰æ˜¯ç¬¬{section_index}ä¸ªç« èŠ‚
   - è¯·æ ¹æ®è¯¥ç« èŠ‚åœ¨æ•´ç¯‡æŠ¥å‘Šä¸­çš„ä½ç½®ã€å†…å®¹é‡è¦æ€§å’Œå¤æ‚åº¦ï¼Œè‡ªä¸»åˆç†å®‰æ’æœ¬ç« èŠ‚çš„å­—æ•°
   - ç¡®ä¿æ•´ç¯‡æŠ¥å‘Šçš„æ€»å­—æ•°æ§åˆ¶åœ¨{total_words}å­—å·¦å³"""
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ€»å­—æ•°ï¼Œä¸é™åˆ¶å­—æ•°
            words_instruction = """3. **å­—æ•°è¦æ±‚**ï¼š
   - è¯·æ ¹æ®æœ¬ç« èŠ‚çš„å†…å®¹é‡è¦æ€§å’Œå¤æ‚åº¦ï¼Œåˆç†å®‰æ’å­—æ•°
   - ç¡®ä¿å†…å®¹å®Œæ•´ã€æœ‰æ·±åº¦"""
        
        # æ„å»ºç³»ç»Ÿæç¤º
        has_level2_titles = len(level2_titles) > 0
        
        if has_level2_titles:
            structure_instruction = """1. **ç« èŠ‚ç»“æ„è¦æ±‚**ï¼š
   - å¿…é¡»åŒ…å«å®Œæ•´çš„ä¸€çº§æ ‡é¢˜ï¼ˆ## æ ‡é¢˜ï¼‰
   - ä¸€çº§æ ‡é¢˜ååº”è¯¥å…ˆå†™ä¸€æ®µæ€»ç»“æ¦‚è¿°ï¼ˆ2-3æ®µï¼‰ï¼Œæ¦‚æ‹¬æœ¬ç« èŠ‚çš„ä¸»è¦å†…å®¹
   - ç„¶åæŒ‰ç…§å¤§çº²ç»™å®šçš„äºŒçº§æ ‡é¢˜ï¼ˆ### æ ‡é¢˜ï¼‰é¡ºåºï¼Œé€ä¸€æ’°å†™å„å°èŠ‚å†…å®¹
   - ä¸¥æ ¼éµå¾ªå¤§çº²çš„äºŒçº§æ ‡é¢˜ç»“æ„ï¼Œä¸å¾—é—æ¼ã€å¢åŠ æˆ–ä¿®æ”¹äºŒçº§æ ‡é¢˜"""
        else:
            structure_instruction = """1. **ç« èŠ‚ç»“æ„è¦æ±‚**ï¼š
   - å¿…é¡»åŒ…å«å®Œæ•´çš„ä¸€çº§æ ‡é¢˜ï¼ˆ## æ ‡é¢˜ï¼‰
   - ä¸€çº§æ ‡é¢˜åç›´æ¥æ’°å†™å†…å®¹ï¼Œä¸éœ€è¦äºŒçº§æ ‡é¢˜
   - å†…å®¹åº”è¯¥å±‚æ¬¡æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†ã€ä¿¡æ¯ä¸°å¯Œ"""
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ¥å‘Šæ’°å†™ä¸“å®¶ï¼Œæ“…é•¿æ’°å†™ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†ã€å†…å®¹ä¸°å¯Œçš„æŠ¥å‘Šç« èŠ‚ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æ’°å†™ï¼š
{structure_instruction}
2. å†…å®¹åº”è¯¥ä¸“ä¸šã€æœ‰æ·±åº¦ã€ä¿¡æ¯ä¸°å¯Œ
{words_instruction}
4. éœ€è¦ä¸ä¹‹å‰æ’°å†™çš„å†…å®¹ä¿æŒè¿è´¯æ€§
5. **æ ¼å¼è¦æ±‚**ï¼š
   - ä½¿ç”¨Markdownæ ¼å¼ï¼Œæ®µè½ä¹‹é—´å¿…é¡»æœ‰ä¸€ä¸ªç©ºè¡Œï¼ˆåŒæ¢è¡Œç¬¦ï¼‰
   - ä¸€çº§æ ‡é¢˜ä½¿ç”¨ ##ï¼ŒäºŒçº§æ ‡é¢˜ä½¿ç”¨ ###ï¼Œä¸€çº§æ ‡é¢˜ä¸‹å…ˆæœ‰æ¦‚è¿°æ®µè½
   - é€‚å½“ä½¿ç”¨åˆ—è¡¨ã€å¼•ç”¨ã€åŠ ç²—ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
   - **æ•°æ®å¯è§†åŒ–ï¼šæ ¹æ®æ•°æ®ç‰¹ç‚¹æ™ºèƒ½é€‰æ‹©è¡¨æ ¼åŠå›¾è¡¨**ï¼š
     * **è¡¨æ ¼**ï¼šé€‚åˆç²¾ç¡®æ•°å€¼ã€å¤šç»´åº¦æ•°æ®ï¼ˆå¦‚ï¼šå›½å®¶Ã—å¹´ä»½Ã—æŒ‡æ ‡ï¼‰ã€æ•°æ®é¡¹è¾ƒå¤šï¼ˆ5ä¸ªä»¥ä¸Šï¼‰ã€‚ä½¿ç”¨Markdownè¡¨æ ¼æ ¼å¼ã€‚**é‡è¦ï¼šå¦‚æœç« èŠ‚ä¸­æœ‰é€‚åˆè¡¨æ ¼å±•ç¤ºçš„æ•°æ®ï¼ˆå¦‚å¯¹æ¯”æ•°æ®ã€ç»Ÿè®¡æ•°æ®ã€åˆ†ç±»æ•°æ®ç­‰ï¼‰ï¼Œå°½é‡ä¸ºæ¯ä¸ªç« èŠ‚åŒ…å«è‡³å°‘ä¸€ä¸ªè¡¨æ ¼ï¼Œä»¥æå‡æ•°æ®çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§ã€‚**
     * **å›¾è¡¨**ï¼šé€‚åˆå±•ç¤ºè¶‹åŠ¿ï¼ˆæŠ˜çº¿å›¾lineï¼‰ã€å æ¯”ï¼ˆé¥¼å›¾pieï¼‰ã€å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾barï¼‰ã€ç›¸å…³æ€§ï¼ˆæ•£ç‚¹å›¾scatterï¼‰ã€‚ä½¿ç”¨æ ¼å¼ï¼š`[CHART:ç±»å‹:æè¿°:ç« èŠ‚æ ‡é¢˜]`ï¼Œä¾‹å¦‚ï¼š`[CHART:bar:2020-2023å¹´å…¨çƒæ ¸ç”µè£…æœºå®¹é‡å¯¹æ¯”:### è£…æœºå®¹é‡åˆ†æ]`ã€‚æ’å…¥ä½ç½®å¿…é¡»æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ˆ## ä¸€çº§æ ‡é¢˜ æˆ– ### äºŒçº§æ ‡é¢˜ï¼‰ï¼Œå›¾è¡¨å°†æ’å…¥åˆ°è¯¥ç« èŠ‚çš„æœ«å°¾ã€‚
     * å›¾è¡¨ä¸€ç« èŠ‚åªèƒ½ä½¿ç”¨ä¸€ä¸ªã€‚è¡¨æ ¼å’Œå›¾è¡¨å¯ä»¥åŒæ—¶ä½¿ç”¨ï¼Œå®ƒä»¬æœåŠ¡äºä¸åŒçš„æ•°æ®å±•ç¤ºéœ€æ±‚ã€‚
6. **å¼•ç”¨æ ‡æ³¨**ï¼š
   - ä¸¥ç¦åœ¨æ­£æ–‡ä¸­ä½¿ç”¨ä»»ä½•å½¢å¼çš„æ–‡å†…å¼•ç”¨æ ‡æ³¨ï¼ˆ[ref_1]ã€[1]ã€[^ref_1] ç­‰ï¼‰
   - åœ¨ç« èŠ‚æœ«å°¾å•ç‹¬ä¸€è¡Œå†™ï¼šCITATIONS: ref_1, ref_3, ref_5ï¼ˆæœªä½¿ç”¨åˆ™å†™ï¼šCITATIONS:ï¼‰
7. ç¡®ä¿å†…å®¹ä¸æŠ¥å‘Šå¤§çº²ä¸€è‡´ï¼Œå¿…é¡»å®Œæ•´ï¼Œæœ‰è¶³å¤Ÿæ·±åº¦å’Œç»†èŠ‚
8. å¦‚æœæä¾›äº†å†å²ç« èŠ‚æˆ–æ£€ç´¢ç»“æœï¼Œéœ€è¦å‚è€ƒå¹¶å……åˆ†æ•´åˆä¿¡æ¯"""

        # æ„å»ºç”¨æˆ·æç¤º
        context_parts = []
        
        # 1. å®Œæ•´å¤§çº²
        context_parts.append(f"## å®Œæ•´æŠ¥å‘Šå¤§çº²ï¼š\n{outline}\n")
        
        # 2. å‰æ–‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        if previous_sections_summary:
            context_parts.append(f"## å‰æ–‡æ‘˜è¦ï¼š\n{previous_sections_summary}\n")
        
        # 3. å†å²ç« èŠ‚å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if history_sections:
            context_parts.append("## ç›¸å…³å†å²ç« èŠ‚å†…å®¹ï¼ˆä¾›å‚è€ƒï¼Œä¿æŒè¿è´¯æ€§ï¼‰ï¼š\n")
            for i, hist_content in enumerate(history_sections, 1):
                context_parts.append(f"### å†å²ç« èŠ‚ {i}ï¼š\n{hist_content}\n")
        
        # 4. æ£€ç´¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if search_results:
            context_parts.append("## æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n")
            for i, result in enumerate(search_results[:10], 1):  # æœ€å¤šä½¿ç”¨å‰10æ¡
                title = result.get("title", "")
                content = result.get("content", "")[:300]  # åªå–å‰300å­—ç¬¦
                source = result.get("source", "æœªçŸ¥æ¥æº")
                ref_id = result.get("ref_id", f"ref_{i}")
                context_parts.append(f"[{i}] [{ref_id}] {title} ({source})\n   {content}...\n")
        
        # 5. å½“å‰ä»»åŠ¡
        words_context = ""
        if total_words:
            words_context = f"""
- **å­—æ•°è¦æ±‚**ï¼š
  * æ•´ç¯‡æŠ¥å‘Šæ€»å­—æ•°çº¦{total_words}å­—ï¼Œå…±{total_sections}ä¸ªç« èŠ‚
  * å½“å‰å·²å†™çº¦{written_words or 0}å­—
  * è¿™æ˜¯ç¬¬{section_index}ä¸ªç« èŠ‚
  * è¯·æ ¹æ®ç« èŠ‚å†…å®¹çš„é‡è¦æ€§å’Œå¤æ‚åº¦è‡ªä¸»åˆç†å®‰æ’å­—æ•°"""
        
        # æ ¼å¼åŒ–äºŒçº§æ ‡é¢˜åˆ—è¡¨
        if level2_titles:
            level2_titles_str = "\n  ".join([f"- {title}" for title in level2_titles])
            level2_section = f"""
- è¯¥ä¸€çº§æ ‡é¢˜ä¸‹åŒ…å«çš„äºŒçº§æ ‡é¢˜ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰æ­¤é¡ºåºæ’°å†™ï¼‰ï¼š
  {level2_titles_str}"""
            writing_instructions = """
**é‡è¦è¯´æ˜**ï¼š
- è¯·æ’°å†™å®Œæ•´çš„ä¸€çº§æ ‡é¢˜ç« èŠ‚å†…å®¹
- å¿…é¡»åŒ…å«ä¸€çº§æ ‡é¢˜ï¼ˆ## {level1_title}ï¼‰
- ä¸€çº§æ ‡é¢˜åå…ˆå†™2-3æ®µæ€»ç»“æ¦‚è¿°ï¼Œæ¦‚æ‹¬æœ¬ç« èŠ‚ä¸»è¦å†…å®¹
- ç„¶åæŒ‰ç…§ä¸Šè¿°äºŒçº§æ ‡é¢˜åˆ—è¡¨çš„é¡ºåºï¼Œé€ä¸€æ’°å†™å„å°èŠ‚ï¼ˆ### äºŒçº§æ ‡é¢˜ï¼‰
- ä¸¥æ ¼éµå¾ªå¤§çº²çš„äºŒçº§æ ‡é¢˜ï¼Œä¸å¾—é—æ¼ã€å¢åŠ æˆ–ä¿®æ”¹"""
        else:
            level2_section = ""
            writing_instructions = """
**é‡è¦è¯´æ˜**ï¼š
- è¯·æ’°å†™å®Œæ•´çš„ä¸€çº§æ ‡é¢˜ç« èŠ‚å†…å®¹
- å¿…é¡»åŒ…å«ä¸€çº§æ ‡é¢˜ï¼ˆ## {level1_title}ï¼‰
- ä¸€çº§æ ‡é¢˜åç›´æ¥æ’°å†™å†…å®¹ï¼Œä¸éœ€è¦äºŒçº§æ ‡é¢˜"""
        
        context_parts.append(f"""## å½“å‰ä»»åŠ¡ï¼š
- ç« èŠ‚ç´¢å¼•ï¼šç¬¬ {section_index} ä¸ªç« èŠ‚ï¼ˆå…±{total_sections if total_sections else '?'}ä¸ªç« èŠ‚ï¼‰
- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}{level2_section}{words_context}
{writing_instructions}
- å†…å®¹åº”è¯¥ä¸“ä¸šã€æœ‰æ·±åº¦ã€æœ‰è¯´æœåŠ›ï¼ŒåŒ…å«å…·ä½“çš„äº‹å®å’Œç»†èŠ‚
- ä¸ä¹‹å‰æ’°å†™çš„å†…å®¹ä¿æŒè¿è´¯æ€§ï¼Œå……åˆ†æ•´åˆæ£€ç´¢ç»“æœä¸­çš„å…³é”®ä¿¡æ¯
- ç¡®ä¿ç« èŠ‚å®Œæ•´ï¼Œæœ‰è¶³å¤Ÿæ·±åº¦

è¯·å¼€å§‹æ’°å†™ï¼š""")

        user_prompt = "\n".join(context_parts)
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"âœï¸  [WritingAgent] è°ƒç”¨ LLM æ’°å†™ç« èŠ‚ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰")
            print(f"{'='*60}")
            print(f"æ¨¡å‹: qwen-plus")
            print(f"ä¸€çº§æ ‡é¢˜: {level1_title}")
            print(f"åŒ…å«äºŒçº§æ ‡é¢˜æ•°: {len(level2_titles)}")
            print(f"æ£€ç´¢ç»“æœæ•°: {len(search_results)}")
            print(f"å†å²ç« èŠ‚æ•°: {len(history_sections)}")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [WritingAgent] LLM å“åº”å®Œæˆ (é•¿åº¦: {len(content)} å­—ç¬¦)")
            
            # æ˜¾ç¤ºæœ€å100ä¸ªå­—ç¬¦ï¼Œç”¨äºæ£€æŸ¥æ˜¯å¦æœ‰CITATIONSè¡Œ
            print(f"ğŸ“„ [WritingAgent] å†…å®¹æœ«å°¾: ...{content[-150:]}")
            
            # è§£æå¼•ç”¨ä¿¡æ¯
            citations_data = self._extract_citations(content, search_results)
            content_without_citations = citations_data["content"]
            used_citations = citations_data["citations"]
            
            print(f"ğŸ“š [WritingAgent] æå–åˆ° {len(used_citations)} ä¸ªå¼•ç”¨")
            if len(used_citations) > 0:
                print(f"ğŸ“š [WritingAgent] å¼•ç”¨è¯¦æƒ…: {[c.get('title', 'unknown')[:30] for c in used_citations]}")
            
            # éªŒè¯ç« èŠ‚å®Œæ•´æ€§ï¼ˆæ£€æŸ¥ä¸€çº§æ ‡é¢˜ï¼‰
            content_without_citations = self._ensure_section_completeness(content_without_citations, level1_title)
            
            # æ¸…ç†æ–‡å†…å¼•ç”¨æ ‡è®°ï¼ˆå¦‚æœæ¨¡å‹ä»ç„¶ç”Ÿæˆäº†ï¼‰
            content_without_citations = self._remove_inline_citations(content_without_citations)
            
            # è§£æå›¾è¡¨éœ€æ±‚æ ‡è®°ï¼ˆä»å†…å®¹ä¸­æå–ï¼‰
            chart_requirement = None
            if enable_chart:
                # è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹å†…å®¹ä¸­æ˜¯å¦æœ‰å›¾è¡¨æ ‡è®°
                import re
                chart_pattern = r'\[CHART:([^:]+):([^:]+):([^\]]+)\]'
                chart_matches = re.findall(chart_pattern, content_without_citations)
                if chart_matches:
                    print(f"ğŸ“Š [WritingAgent] åœ¨åŸå§‹å†…å®¹ä¸­æ‰¾åˆ° {len(chart_matches)} ä¸ªå›¾è¡¨æ ‡è®°")
                else:
                    print(f"âš ï¸  [WritingAgent] åŸå§‹å†…å®¹ä¸­æœªæ‰¾åˆ°å›¾è¡¨æ ‡è®° [CHART:...]")
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç±»ä¼¼çš„æ ‡è®°ï¼ˆå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜ï¼‰
                    similar_patterns = [
                        r'\[CHART[^\]]*\]',
                        r'CHART[:\s]',
                        r'å›¾è¡¨[:\s]',
                    ]
                    for pattern in similar_patterns:
                        matches = re.findall(pattern, content_without_citations, re.IGNORECASE)
                        if matches:
                            print(f"  âš ï¸  æ‰¾åˆ°ç±»ä¼¼æ ‡è®°: {pattern} -> {matches[:3]}...")
                
                chart_data = self._extract_chart_requirement(content_without_citations)
                if chart_data:
                    chart_requirement = chart_data["requirement"]
                    content_without_citations = chart_data["content"]  # ç§»é™¤æ ‡è®°åçš„å†…å®¹
                    if chart_requirement:
                        print(f"ğŸ“Š [WritingAgent] æ£€æµ‹åˆ°å›¾è¡¨éœ€æ±‚: {chart_requirement.get('chart_type', 'unknown')}")
                else:
                    print(f"âš ï¸  [WritingAgent] _extract_chart_requirement è¿”å› Noneï¼ˆæœªæ‰¾åˆ°æœ‰æ•ˆæ ‡è®°ï¼‰")
            
            return {
                "content": content_without_citations,
                "citations": used_citations,
                "chart_requirement": chart_requirement
            }
            
        except Exception as e:
            error_msg = str(e)
            if "API key" in error_msg or "401" in error_msg:
                raise WritingAgentError("API Key è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ DASHSCOPE_API_KEY é…ç½®") from e
            raise WritingAgentError(f"æ’°å†™ç« èŠ‚å¤±è´¥: {error_msg}") from e
    
    def _extract_citations(self, content: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä»å†…å®¹ä¸­æå–å¼•ç”¨ä¿¡æ¯
        
        Args:
            content: LLM ç”Ÿæˆçš„å†…å®¹ï¼ˆåŒ…å« CITATIONS è¡Œï¼‰
            search_results: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆåŒ…å« ref_idï¼‰
        
        Returns:
            {
                "content": str,  # ç§»é™¤ CITATIONS è¡Œåçš„å†…å®¹
                "citations": List[Dict]  # å®é™…ä½¿ç”¨çš„å¼•ç”¨åˆ—è¡¨
            }
        """
        # æŸ¥æ‰¾ CITATIONS: è¡Œï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå…è®¸å‰åæœ‰ç©ºæ ¼ï¼‰
        import re
        citations_line = ""
        content_lines = content.split("\n")
        citations_line_index = -1
        
        # ä»åå¾€å‰æ‰¾ CITATIONS è¡Œ
        for i in range(len(content_lines) - 1, -1, -1):
            line = content_lines[i].strip()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
            # 1. CITATIONS: ref_1, ref_2
            # 2. [CITATIONS: ref_1, ref_2]
            if re.search(r'citations\s*:', line, re.IGNORECASE):
                citations_line = line
                citations_line_index = i
                print(f"ğŸ” [_extract_citations] åœ¨ç¬¬ {i} è¡Œæ‰¾åˆ° CITATIONS: {line}")
                # ç§»é™¤è¿™ä¸€è¡ŒåŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
                content_lines = content_lines[:i]
                break
        
        if citations_line_index == -1:
            print(f"âš ï¸  [_extract_citations] æœªæ‰¾åˆ° CITATIONS è¡Œï¼ŒLLMå¯èƒ½æ²¡æœ‰éµå¾ªæŒ‡ä»¤")
            # æ˜¾ç¤ºæœ€åå‡ è¡Œï¼Œå¸®åŠ©è°ƒè¯•
            print(f"ğŸ“„ [_extract_citations] å†…å®¹æœ€å5è¡Œ:")
            for line in content_lines[-5:]:
                print(f"  | {line}")
        
        # é‡æ–°ç»„è£…å†…å®¹ï¼ˆä¸åŒ…å« CITATIONS è¡Œï¼‰
        clean_content = "\n".join(content_lines).strip()
        
        # è§£æå¼•ç”¨çš„ ref_id
        used_ref_ids = []
        if citations_line:
            # æå– "CITATIONS:" åé¢çš„å†…å®¹ï¼Œæ”¯æŒ [CITATIONS: ...] æ ¼å¼
            # å…ˆç§»é™¤æ–¹æ‹¬å·ï¼Œå†æå– CITATIONS: åé¢çš„å†…å®¹
            citations_part = re.sub(r'^\[?\s*citations\s*:\s*', '', citations_line, flags=re.IGNORECASE)
            citations_part = re.sub(r'\]?\s*$', '', citations_part).strip()
            if citations_part:
                # æŒ‰é€—å·åˆ†å‰²ï¼Œæ¸…ç†ç©ºæ ¼
                used_ref_ids = [ref_id.strip() for ref_id in citations_part.split(",") if ref_id.strip()]
        
        # æ ¹æ® ref_id è¿‡æ»¤ search_results
        used_citations = []
        for result in search_results:
            ref_id = result.get("ref_id", "")
            if ref_id in used_ref_ids:
                # æå–éœ€è¦çš„å­—æ®µ
                citation = {
                    "source": result.get("source", ""),
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "filename": result.get("filename", ""),
                    "content": result.get("content", "")[:200]  # ä¿å­˜æ‘˜è¦
                }
                used_citations.append(citation)
        
        print(f"ğŸ“ [_extract_citations] æ‰¾åˆ° CITATIONS è¡Œ: {citations_line}")
        print(f"ğŸ“ [_extract_citations] ä½¿ç”¨çš„ ref_id: {used_ref_ids}")
        print(f"ğŸ“ [_extract_citations] åŒ¹é…åˆ°çš„å¼•ç”¨: {len(used_citations)} ä¸ª")
        
        return {
            "content": clean_content,
            "citations": used_citations
        }
    
    def _ensure_section_completeness(self, content: str, level1_title: str) -> str:
        """
        ç¡®ä¿ç« èŠ‚å®Œæ•´æ€§ï¼ˆæ£€æŸ¥ä¸€çº§æ ‡é¢˜ï¼‰
        
        Args:
            content: ç”Ÿæˆçš„å†…å®¹
            level1_title: ä¸€çº§æ ‡é¢˜
        
        Returns:
            å®Œæ•´çš„ç« èŠ‚å†…å®¹
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸€çº§æ ‡é¢˜
        if f"## {level1_title}" not in content and f"#{level1_title}" not in content:
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œæ·»åŠ æ ‡é¢˜
            content = f"## {level1_title}\n\n{content}"
        
        # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
        if not content.strip():
            raise WritingAgentError("ç”Ÿæˆçš„ç« èŠ‚å†…å®¹ä¸ºç©º")
        
        return content
    
    def _extract_chart_requirement(self, content: str) -> Optional[Dict[str, Any]]:
        """
        ä»å†…å®¹ä¸­æå–å›¾è¡¨éœ€æ±‚æ ‡è®°
        
        Args:
            content: ç« èŠ‚å†…å®¹ï¼ˆå¯èƒ½åŒ…å« [CHART:ç±»å‹:æè¿°:ç« èŠ‚æ ‡é¢˜] æ ‡è®°ï¼‰
        
        Returns:
            å¦‚æœæ‰¾åˆ°å›¾è¡¨æ ‡è®°ï¼Œè¿”å›ï¼š
            {
                "content": str,  # ç§»é™¤æ ‡è®°åçš„å†…å®¹
                "requirement": {
                    "chart_type": str,  # "bar", "line", "pie", "scatter"
                    "chart_description": str,  # å›¾è¡¨æè¿°
                    "insert_after_section": str,  # ç« èŠ‚æ ‡é¢˜ï¼ˆ## ä¸€çº§æ ‡é¢˜ æˆ– ### äºŒçº§æ ‡é¢˜ï¼‰
                    "chart_width": float,  # é»˜è®¤10
                    "chart_height": float,  # é»˜è®¤6
                }
            }
            å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å› None
        """
        import re
        
        # åŒ¹é… [CHART:ç±»å‹:æè¿°:ç« èŠ‚æ ‡é¢˜] æ ¼å¼
        pattern = r'\[CHART:([^:]+):([^:]+):([^\]]+)\]'
        match = re.search(pattern, content)
        
        if not match:
            return None
        
        chart_type = match.group(1).strip()
        chart_description = match.group(2).strip()
        insert_after_section = match.group(3).strip()
        
        # éªŒè¯å›¾è¡¨ç±»å‹
        valid_types = ["bar", "line", "pie", "scatter"]
        if chart_type not in valid_types:
            print(f"âš ï¸  [WritingAgent] æ— æ•ˆçš„å›¾è¡¨ç±»å‹: {chart_type}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹ bar")
            chart_type = "bar"
        
        # éªŒè¯ç« èŠ‚æ ‡é¢˜æ ¼å¼ï¼ˆåº”è¯¥æ˜¯ ## æˆ– ### å¼€å¤´çš„æ ‡é¢˜ï¼‰
        if not (insert_after_section.startswith("##") or insert_after_section.startswith("###")):
            print(f"âš ï¸  [WritingAgent] æ’å…¥ä½ç½®ä¸æ˜¯æœ‰æ•ˆçš„ç« èŠ‚æ ‡é¢˜æ ¼å¼: {insert_after_section}ï¼Œå°†å°è¯•åŒ¹é…")
        
        # ç§»é™¤æ ‡è®°
        content_without_marker = content.replace(match.group(0), "").strip()
        
        return {
            "content": content_without_marker,
            "requirement": {
                "chart_type": chart_type,
                "chart_description": chart_description,
                "insert_after_section": insert_after_section,
                "chart_width": 10.0,
                "chart_height": 6.0,
            }
        }
    
    def _remove_inline_citations(self, content: str) -> str:
        """
        ç§»é™¤å†…å®¹ä¸­çš„æ–‡å†…å¼•ç”¨æ ‡è®°
        
        Args:
            content: ç« èŠ‚å†…å®¹ï¼ˆå¯èƒ½åŒ…å«æ–‡å†…å¼•ç”¨æ ‡è®°ï¼‰
        
        Returns:
            ç§»é™¤å¼•ç”¨æ ‡è®°åçš„å†…å®¹
        """
        import re
        
        # ç§»é™¤å„ç§æ ¼å¼çš„æ–‡å†…å¼•ç”¨ï¼š
        # [ref_1], [ref_2], [1], [2], [^ref_1], [^1] ç­‰
        patterns = [
            r'\[ref_\d+\]',           # [ref_1], [ref_2]
            r'\[\^ref_\d+\]',          # [^ref_1], [^ref_2]
            r'\[\^\d+\]',              # [^1], [^2]
            r'(?<!\[)\[\d+\](?!\()',  # [1], [2] (ä½†ä¸åŒ¹é…é“¾æ¥æ ¼å¼ [[1]](url))
        ]
        
        cleaned_content = content
        for pattern in patterns:
            cleaned_content = re.sub(pattern, '', cleaned_content)
        
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼ï¼ˆå¼•ç”¨æ ‡è®°ç§»é™¤åå¯èƒ½ç•™ä¸‹çš„ï¼‰ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
        # åªæ¸…ç†ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ï¼Œä¸æ¸…ç†æ¢è¡Œç¬¦
        cleaned_content = re.sub(r'[ \t]+', ' ', cleaned_content)  # å¤šä¸ªç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦åˆå¹¶ä¸ºä¸€ä¸ªç©ºæ ¼ï¼ˆä¸åŒ…å«æ¢è¡Œç¬¦ï¼‰
        cleaned_content = re.sub(r'[ \t]+([ã€‚ï¼Œã€ï¼›ï¼š])', r'\1', cleaned_content)  # æ ‡ç‚¹å‰çš„ç©ºæ ¼
        
        return cleaned_content
    
    def generate_search_queries(
        self,
        section: Dict[str, Any],
        outline: Dict[str, Any],
        requirement: Optional[str] = None
    ) -> List[str]:
        """
        ç”Ÿæˆæ£€ç´¢è¯­å¥ï¼ˆåœ¨å‡†å¤‡é˜¶æ®µè°ƒç”¨ï¼Œä¸ºæ•´ä¸ªä¸€çº§ç« èŠ‚ç”Ÿæˆï¼‰
        
        æ ¹æ®ç« èŠ‚é¢„ä¼°å­—æ•°å’ŒäºŒçº§æ ‡é¢˜æ•°é‡ç¡®å®šæ£€ç´¢è¯­å¥æ•°é‡ï¼š
        - åŸºç¡€æŸ¥è¯¢æ•°ï¼šæŒ‰æ€»å­—æ•°å’Œç« èŠ‚æ•°è®¡ç®—
        - æ¯ä¸ªäºŒçº§æ ‡é¢˜è‡³å°‘1ä¸ªæŸ¥è¯¢
        
        Args:
            section: å½“å‰ç« èŠ‚ä¿¡æ¯ï¼ˆåŒ…å« level1_title å’Œ level2_titlesï¼‰
            outline: å¤§çº²ä¿¡æ¯ï¼ˆåŒ…å« estimated_wordsï¼‰
            requirement: æ–‡ç« æ•´ä½“éœ€æ±‚ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ£€ç´¢è¯­å¥åˆ—è¡¨
        """
        # è®¡ç®—å½“å‰ç« èŠ‚çš„é¢„ä¼°å­—æ•°
        # æ–¹æ³•ï¼šæ€»é¢„ä¼°å­—æ•° / ä¸€çº§ç« èŠ‚æ€»æ•°
        estimated_words = outline.get("estimated_words", 0)
        total_sections = len(outline.get("sections", []))
        if total_sections > 0:
            avg_words_per_level1 = estimated_words / total_sections
        else:
            avg_words_per_level1 = 500  # é»˜è®¤500å­—
        
        # è·å–å½“å‰ç« èŠ‚çš„äºŒçº§æ ‡é¢˜æ•°é‡
        level2_titles = section.get("level2_titles", [])
        num_level2 = len(level2_titles)
        
        # æ ¹æ®é¢„ä¼°å­—æ•°å’ŒäºŒçº§æ ‡é¢˜æ•°é‡ç¡®å®šæ£€ç´¢è¯­å¥æ•°é‡
        # ç­–ç•¥ï¼šæ¯ä¸ªäºŒçº§æ ‡é¢˜è‡³å°‘1ä¸ªæŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰äºŒçº§æ ‡é¢˜åˆ™é»˜è®¤3ä¸ª
        if num_level2 > 0:
            num_queries = max(3, num_level2)
        else:
            # æ²¡æœ‰äºŒçº§æ ‡é¢˜æ—¶ï¼Œæ ¹æ®é¢„ä¼°å­—æ•°å†³å®š
            num_queries = 3 if avg_words_per_level1 < 800 else 4
        
        level1_title = section.get("level1_title", "")
        
        # ç¡®ä¿ level1_title ä¸ä¸ºç©ºï¼ˆé¿å…è¾“å…¥é•¿åº¦é”™è¯¯ï¼‰
        if not level1_title:
            level1_title = "ç« èŠ‚"
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ£€ç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ç« èŠ‚ä¿¡æ¯ç”Ÿæˆç²¾ç¡®ã€æœ‰æ•ˆçš„æ£€ç´¢æŸ¥è¯¢è¯­å¥ã€‚

**ä»»åŠ¡**ï¼š
ä¸ºå½“å‰ä¸€çº§ç« èŠ‚ç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼Œç”¨äºä»çŸ¥è¯†åº“å’Œç½‘ç»œä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚

**è¦æ±‚**ï¼š
1. **åŒ…å«å…³é”®äººç‰©ã€äº‹ä»¶ã€ä¸»é¢˜**ï¼šå¦‚æœç« èŠ‚æ¶‰åŠç‰¹å®šäººç‰©æˆ–ä¸»é¢˜ï¼ŒæŸ¥è¯¢è¯­å¥å¿…é¡»åŒ…å«ç›¸å…³åç§°
2. **æŸ¥è¯¢è¯­å¥åº”è¯¥è¦†ç›–ç« èŠ‚çš„ä¸åŒç»´åº¦**ï¼š{num_queries} ä¸ªæŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦æ£€ç´¢ï¼Œå°½é‡è¦†ç›–è¯¥ä¸€çº§ç« èŠ‚ä¸‹æ‰€æœ‰äºŒçº§æ ‡é¢˜æ¶‰åŠçš„å†…å®¹
3. **é€‚åˆå‘é‡æ£€ç´¢å’Œç½‘ç»œæœç´¢**ï¼šæŸ¥è¯¢è¯­å¥åº”è¯¥æ¸…æ™°ã€å…·ä½“ï¼Œä½¿ç”¨å…³é”®è¯ç»„åˆï¼Œèƒ½å¤ŸåŒ¹é…åˆ°ç›¸å…³å†…å®¹
4. **ç®€æ´æ˜äº†**ï¼šæ¯ä¸ªæŸ¥è¯¢è¯­å¥é•¿åº¦æ§åˆ¶åœ¨25å­—ä»¥å†…ï¼ŒåŒ…å«3-6ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œé¿å…å†—ä½™
5. **äº’è¡¥æ€§**ï¼šå¤šä¸ªæŸ¥è¯¢åº”è¯¥äº’è¡¥ï¼Œå…±åŒè¦†ç›–ç« èŠ‚æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ç‚¹

**è¾“å‡ºæ ¼å¼**ï¼š
è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ•°ç»„ï¼ŒåŒ…å« {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼š
["æŸ¥è¯¢è¯­å¥1", "æŸ¥è¯¢è¯­å¥2", "æŸ¥è¯¢è¯­å¥3"]

**ç¤ºä¾‹**ï¼š
- ä¸€çº§ç« èŠ‚ï¼š"æ ¸èƒ½å‘å±•ç°çŠ¶"ï¼ŒåŒ…å«äºŒçº§æ ‡é¢˜ï¼š["è£…æœºå®¹é‡åˆ†æ", "ä¸»è¦å›½å®¶æ”¿ç­–å¯¹æ¯”"]ï¼Œéœ€è¦3ä¸ªæŸ¥è¯¢
  è¾“å‡ºï¼š["æ ¸èƒ½ è£…æœºå®¹é‡ å…¨çƒ", "æ ¸ç”µæ”¿ç­– ä¸»è¦å›½å®¶", "æ ¸èƒ½å‘å±• ç°çŠ¶åˆ†æ"]""".format(num_queries=num_queries)

        # æ ¼å¼åŒ–äºŒçº§æ ‡é¢˜åˆ—è¡¨
        level2_titles_str = "ã€".join(level2_titles[:5])  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
        if len(level2_titles) > 5:
            level2_titles_str += f"...ï¼ˆå…±{len(level2_titles)}ä¸ªï¼‰"

        user_prompt_parts = [
            f"ç« èŠ‚ä¿¡æ¯ï¼š",
            f"- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}",
            f"- åŒ…å«äºŒçº§æ ‡é¢˜ï¼š{level2_titles_str}",
        ]
        
        if requirement:
            user_prompt_parts.append(f"\næ–‡ç« æ•´ä½“ä¸»é¢˜ï¼š{requirement}")
        
        user_prompt_parts.append(f"\né¢„ä¼°å­—æ•°ï¼š{avg_words_per_level1:.0f}å­—ï¼ˆéœ€è¦ç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼‰")
        user_prompt_parts.append("\nè¯·ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰ï¼š")
        
        user_prompt = "\n".join(user_prompt_parts)
        
        # ç¡®ä¿ prompt é•¿åº¦åœ¨å…è®¸èŒƒå›´å†…ï¼ˆDashScope é™åˆ¶ï¼š1-2048 å­—ç¬¦ï¼‰
        MAX_TOTAL_LENGTH = 2000  # ç•™ä¸€äº›ä½™é‡
        system_length = len(system_prompt)
        user_length = len(user_prompt)
        total_length = system_length + user_length
        
        if total_length > MAX_TOTAL_LENGTH:
            # éœ€è¦æˆªæ–­ user_prompt
            max_user_length = MAX_TOTAL_LENGTH - system_length - 100  # å†ç•™ä¸€äº›ä½™é‡
            if max_user_length > 0:
                # ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œæˆªæ–­ requirementï¼ˆå¦‚æœå¤ªé•¿ï¼‰
                essential_parts = [
                    f"ç« èŠ‚ä¿¡æ¯ï¼š",
                    f"- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}",
                    f"- åŒ…å«äºŒçº§æ ‡é¢˜ï¼š{level2_titles_str}",
                    f"\né¢„ä¼°å­—æ•°ï¼š{avg_words_per_level1:.0f}å­—ï¼ˆéœ€è¦ç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼‰",
                    "\nè¯·ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰ï¼š"
                ]
                if requirement and len(requirement) < 100:
                    essential_parts.insert(3, f"\næ–‡ç« æ•´ä½“ä¸»é¢˜ï¼š{requirement}")
                
                user_prompt = "\n".join(essential_parts)
                if len(user_prompt) > max_user_length:
                    user_prompt = user_prompt[:max_user_length] + "\n[å†…å®¹å·²æˆªæ–­]"
        
        # ç¡®ä¿ prompt ä¸ä¸ºç©º
        if not user_prompt or len(user_prompt.strip()) == 0:
            user_prompt = f"""ç« èŠ‚ä¿¡æ¯ï¼š
- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}
- åŒ…å«äºŒçº§æ ‡é¢˜ï¼š{level2_titles_str}

è¯·ç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰ï¼š"""
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"ğŸ” [WritingAgent] ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼ˆä¸€çº§ç« èŠ‚ï¼‰")
            print(f"{'='*60}")
            print(f"ä¸€çº§æ ‡é¢˜: {level1_title}")
            print(f"äºŒçº§æ ‡é¢˜æ•°: {num_level2}")
            print(f"é¢„ä¼°å­—æ•°: {avg_words_per_level1:.0f}å­—")
            print(f"éœ€è¦ç”Ÿæˆ: {num_queries} ä¸ªæŸ¥è¯¢è¯­å¥")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [WritingAgent] LLM å“åº”å®Œæˆ")
            
            # æå–JSON
            content = self._extract_json_array(content)
            
            # è§£æJSON
            import json
            queries = json.loads(content)
            
            # éªŒè¯å’Œè§„èŒƒåŒ–
            if not isinstance(queries, list):
                raise ValueError("è¿”å›çš„ä¸æ˜¯æ•°ç»„æ ¼å¼")
            
            queries = [str(q).strip() for q in queries if q]
            queries = queries[:num_queries]  # ç¡®ä¿ä¸è¶…è¿‡éœ€è¦çš„æ•°é‡
            
            # å¦‚æœæ•°é‡ä¸è¶³ï¼Œè¡¥å……é»˜è®¤æŸ¥è¯¢
            while len(queries) < num_queries:
                queries.append(level1_title)
            
            print(f"âœ… ç”Ÿæˆçš„æ£€ç´¢æŸ¥è¯¢è¯­å¥: {queries}")
            
            return queries
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢")
            # å›é€€åˆ°é»˜è®¤æŸ¥è¯¢
            default_query = level1_title
            return [default_query] * num_queries
    
    def generate_search_queries_for_missing_points(
        self,
        section: Dict[str, Any],
        missing_points: List[str],
        num_queries: int,
        requirement: Optional[str] = None
    ) -> List[str]:
        """
        åŸºäºç¼ºå¤±ç‚¹ç”Ÿæˆæ£€ç´¢è¯­å¥
        
        Args:
            section: å½“å‰ç« èŠ‚ä¿¡æ¯
            missing_points: ç¼ºå¤±çš„ä¿¡æ¯ç‚¹åˆ—è¡¨
            num_queries: éœ€è¦ç”Ÿæˆçš„æŸ¥è¯¢è¯­å¥æ•°é‡
            requirement: æ–‡ç« æ•´ä½“éœ€æ±‚ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ£€ç´¢è¯­å¥åˆ—è¡¨
        """
        level1_title = section.get("level1_title", "")
        level2_titles = section.get("level2_titles", [])
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ£€ç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ä¿¡æ¯ç¼ºå¤±ç‚¹ç”Ÿæˆç²¾ç¡®çš„æ£€ç´¢æŸ¥è¯¢è¯­å¥ã€‚

**ä»»åŠ¡**ï¼š
æ ¹æ®ç« èŠ‚ä¿¡æ¯å’Œç¼ºå¤±çš„ä¿¡æ¯ç‚¹ï¼Œç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼Œç”¨äºæ£€ç´¢ç¼ºå¤±çš„ä¿¡æ¯ã€‚

**è¦æ±‚**ï¼š
1. **é’ˆå¯¹ç¼ºå¤±ç‚¹**ï¼šæ¯ä¸ªæŸ¥è¯¢è¯­å¥åº”è¯¥é’ˆå¯¹å…·ä½“çš„ç¼ºå¤±ä¿¡æ¯ç‚¹ï¼Œä¼˜å…ˆè¦†ç›–æœ€é‡è¦çš„ç¼ºå¤±ç‚¹
2. **åŒ…å«å…³é”®äººç‰©ã€äº‹ä»¶ã€ä¸»é¢˜**ï¼šå¦‚æœç« èŠ‚æ¶‰åŠç‰¹å®šäººç‰©ï¼ŒæŸ¥è¯¢è¯­å¥å¿…é¡»åŒ…å«è¯¥äººç‰©åå­—
3. **æŸ¥è¯¢è¯­å¥åº”è¯¥è¦†ç›–ä¸åŒçš„ç¼ºå¤±ç‚¹**ï¼š{num_queries} ä¸ªæŸ¥è¯¢åº”è¯¥å°½å¯èƒ½è¦†ç›–ä¸åŒçš„ç¼ºå¤±ç‚¹ï¼Œå¯ä»¥ä¸€ä¸ªæŸ¥è¯¢è¦†ç›–å¤šä¸ªç›¸å…³çš„ç¼ºå¤±ç‚¹
4. **é€‚åˆå‘é‡æ£€ç´¢å’Œç½‘ç»œæœç´¢**ï¼šæŸ¥è¯¢è¯­å¥åº”è¯¥æ¸…æ™°ã€å…·ä½“ï¼Œä½¿ç”¨å…³é”®è¯ç»„åˆ
5. **ç®€æ´æ˜äº†**ï¼šæ¯ä¸ªæŸ¥è¯¢è¯­å¥é•¿åº¦æ§åˆ¶åœ¨25å­—ä»¥å†…ï¼ŒåŒ…å«3-6ä¸ªæ ¸å¿ƒå…³é”®è¯
6. **äº’è¡¥æ€§**ï¼šå¤šä¸ªæŸ¥è¯¢åº”è¯¥äº’è¡¥ï¼Œå…±åŒè¦†ç›–æ‰€æœ‰ç¼ºå¤±çš„ä¿¡æ¯ç‚¹

**è¾“å‡ºæ ¼å¼**ï¼š
è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ•°ç»„ï¼ŒåŒ…å« {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼š
["æŸ¥è¯¢è¯­å¥1", "æŸ¥è¯¢è¯­å¥2", "æŸ¥è¯¢è¯­å¥3"] """.format(num_queries=num_queries)

        # æ ¼å¼åŒ–äºŒçº§æ ‡é¢˜åˆ—è¡¨
        level2_titles_str = "ã€".join(level2_titles[:5])  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
        if len(level2_titles) > 5:
            level2_titles_str += f"...ï¼ˆå…±{len(level2_titles)}ä¸ªï¼‰"

        user_prompt_parts = [
            f"ç« èŠ‚ä¿¡æ¯ï¼š",
            f"- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}",
            f"- åŒ…å«äºŒçº§æ ‡é¢˜ï¼š{level2_titles_str}",
        ]
        
        if requirement:
            user_prompt_parts.append(f"\næ–‡ç« æ•´ä½“ä¸»é¢˜ï¼š{requirement}")
        
        user_prompt_parts.append(f"\nç¼ºå¤±çš„ä¿¡æ¯ç‚¹ï¼š")
        for i, point in enumerate(missing_points, 1):
            user_prompt_parts.append(f"{i}. {point}")
        
        user_prompt_parts.append(f"\néœ€è¦ç”Ÿæˆ {num_queries} ä¸ªæ£€ç´¢æŸ¥è¯¢è¯­å¥æ¥æ£€ç´¢è¿™äº›ç¼ºå¤±çš„ä¿¡æ¯ã€‚")
        user_prompt_parts.append("\nè¯·ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰ï¼š")
        
        user_prompt = "\n".join(user_prompt_parts)
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"ğŸ” [WritingAgent] åŸºäºç¼ºå¤±ç‚¹ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥")
            print(f"{'='*60}")
            print(f"ä¸€çº§æ ‡é¢˜: {level1_title}")
            print(f"ç¼ºå¤±ç‚¹: {missing_points}")
            print(f"éœ€è¦ç”Ÿæˆ: {num_queries} ä¸ªæŸ¥è¯¢è¯­å¥")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [WritingAgent] LLM å“åº”å®Œæˆ")
            
            # æå–JSON
            content = self._extract_json_array(content)
            
            # è§£æJSON
            import json
            queries = json.loads(content)
            
            # éªŒè¯å’Œè§„èŒƒåŒ–
            if not isinstance(queries, list):
                raise ValueError("è¿”å›çš„ä¸æ˜¯æ•°ç»„æ ¼å¼")
            
            queries = [str(q).strip() for q in queries if q]
            queries = queries[:num_queries]  # ç¡®ä¿ä¸è¶…è¿‡éœ€è¦çš„æ•°é‡
            
            # å¦‚æœæ•°é‡ä¸è¶³ï¼Œè¡¥å……é»˜è®¤æŸ¥è¯¢
            while len(queries) < num_queries:
                queries.append(level1_title)
            
            print(f"âœ… ç”Ÿæˆçš„æ£€ç´¢æŸ¥è¯¢è¯­å¥: {queries}")
            
            return queries
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢")
            # å›é€€åˆ°é»˜è®¤æŸ¥è¯¢
            default_query = level1_title
            return [default_query] * num_queries
    
    def _extract_json_array(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSONæ•°ç»„"""
        text = text.strip()
        
        # å¦‚æœåŒ…å«markdownä»£ç å—
        if text.startswith("```"):
            lines = text.split("\n")
            json_start = 0
            json_end = len(lines)
            
            for i, line in enumerate(lines):
                if line.strip().startswith("```"):
                    if json_start == 0:
                        json_start = i + 1
                    else:
                        json_end = i
                        break
            
            text = "\n".join(lines[json_start:json_end])
        
        # æŸ¥æ‰¾JSONæ•°ç»„
        start_idx = text.find("[")
        end_idx = text.rfind("]")
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            text = text[start_idx:end_idx + 1]
        
            return text.strip()
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSONï¼ˆç”¨äºä¿¡æ¯å……è¶³æ€§è¯„ä¼°ï¼‰"""
        text = text.strip()
        
        # å¦‚æœåŒ…å«markdownä»£ç å—
        if text.startswith("```"):
            lines = text.split("\n")
            json_start = 0
            json_end = len(lines)
            
            for i, line in enumerate(lines):
                if line.strip().startswith("```"):
                    if json_start == 0:
                        json_start = i + 1
                    else:
                        json_end = i
                        break
            
            text = "\n".join(lines[json_start:json_end])
        
        # æŸ¥æ‰¾JSONå¯¹è±¡
        start_idx = text.find("{")
        end_idx = text.rfind("}")
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            text = text[start_idx:end_idx + 1]
        
        return text.strip()
    
    def evaluate_info_sufficiency(
        self,
        section: Dict[str, Any],
        search_results: List[Dict[str, Any]],
        history_sections: List[str],
        outline: str
    ) -> Dict[str, Any]:
        """
        åˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……è¶³ï¼ˆç”¨äºå†³å®šæ˜¯å¦éœ€è¦é¢å¤–æ£€ç´¢ï¼‰
        
        Args:
            section: å½“å‰ç« èŠ‚ä¿¡æ¯
            search_results: æ£€ç´¢ç»“æœåˆ—è¡¨
            history_sections: å†å²ç« èŠ‚å†…å®¹åˆ—è¡¨
            outline: å®Œæ•´å¤§çº²
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸:
            {
                "sufficient": bool,  # ä¿¡æ¯æ˜¯å¦å……è¶³
                "missing_points": List[str]  # ç¼ºå¤±çš„ä¿¡æ¯ç‚¹ï¼ˆå¦‚æœä¸è¶³ï¼‰
            }
        """
        level1_title = section.get("level1_title", "")
        level2_title = section.get("level2_title", "")
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¿¡æ¯å……è¶³æ€§è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­å½“å‰æ”¶é›†åˆ°çš„ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿæ”¯æ’‘æŸä¸ªç« èŠ‚çš„å†™ä½œã€‚

**è¯„ä¼°æ ‡å‡†**ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š
1. **ä¸»é¢˜ç›¸å…³æ€§ï¼ˆæœ€é‡è¦ï¼‰**ï¼šä¿¡æ¯æ˜¯å¦ä¸ç« èŠ‚ä¸»é¢˜é«˜åº¦ç›¸å…³ï¼Œç›´æ¥æ¶‰åŠç« èŠ‚çš„æ ¸å¿ƒå†…å®¹
2. **å¤§ä½“è¦†ç›–åº¦**ï¼šä¿¡æ¯æ˜¯å¦èƒ½å¤Ÿå¤§ä½“è¦†ç›–ç« èŠ‚çš„ä¸»è¦ä¿¡æ¯ç‚¹ï¼Œä¸è¦æ±‚æ¯ä¸ªç»†èŠ‚éƒ½é½å…¨
3. **ä¿¡æ¯è´¨é‡**ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®ã€æœ‰ç”¨ï¼Œæ¥è‡ªå¯ä¿¡æ¥æº
4. **å†™ä½œå¯è¡Œæ€§**ï¼šæ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ”¯æ’‘æ’°å†™ä¸€ä¸ªå®Œæ•´çš„ã€æœ‰é€»è¾‘çš„ç« èŠ‚

**è¯„ä¼°åŸåˆ™**ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰ï¼š
- **é‡ç‚¹çœ‹è¦†ç›–èŒƒå›´ï¼Œä¸è¦æ±‚ç»†èŠ‚é½å…¨**ï¼šåªè¦ä¿¡æ¯èƒ½å¤Ÿå¤§ä½“è¦†ç›–ç« èŠ‚çš„ä¸»è¦ä¿¡æ¯ç‚¹å³å¯ï¼Œä¸éœ€è¦æ‰€æœ‰å…·ä½“ç»†èŠ‚ï¼ˆå¦‚å…·ä½“æ—¶é—´ã€å…·ä½“é‡‘é¢ã€å…·ä½“åç§°ç­‰ï¼‰éƒ½é½å…¨
- **é‡è§†æ•´ä½“å¯ç”¨æ€§**ï¼šå³ä½¿æŸäº›å…·ä½“ç»†èŠ‚ç¼ºå¤±ï¼Œä½†å¦‚æœæ•´ä½“ä¿¡æ¯è¶³å¤Ÿæ”¯æ’‘å†™ä½œï¼Œåº”è¯¥è®¤ä¸ºä¿¡æ¯å……è¶³
- **åªå…³æ³¨å…³é”®ç¼ºå¤±**ï¼šåªæœ‰å½“å…³é”®çš„ä¸»è¦ä¿¡æ¯ç‚¹å®Œå…¨ç¼ºå¤±æ—¶ï¼Œæ‰è®¤ä¸ºä¿¡æ¯ä¸è¶³
- **ä¿¡æ¯å……è¶³çš„æ ‡å‡†**ï¼šå¦‚æœæ£€ç´¢ç»“æœèƒ½å¤Ÿè¦†ç›–ç« èŠ‚ä¸»é¢˜çš„ä¸»è¦æ–¹é¢ï¼Œæœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ”¯æ’‘æ’°å†™ä¸€ä¸ªå®Œæ•´ã€æœ‰é€»è¾‘çš„ç« èŠ‚ï¼Œå°±åº”è¯¥åˆ¤æ–­ä¸ºå……è¶³

**è¾“å‡ºæ ¼å¼**ï¼š
è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼š
{
  "sufficient": true/false,
  "missing_points": ["ç¼ºå¤±ç‚¹1", "ç¼ºå¤±ç‚¹2", ...]
}

å¦‚æœä¿¡æ¯å……è¶³ï¼ˆèƒ½å¤Ÿå¤§ä½“è¦†ç›–ç« èŠ‚ä¸»è¦ä¿¡æ¯ç‚¹ï¼‰ï¼Œ`sufficient` ä¸º `true`ï¼Œ`missing_points` ä¸ºç©ºæ•°ç»„ `[]`ã€‚
å¦‚æœä¿¡æ¯ä¸è¶³ï¼ˆå…³é”®çš„ä¸»è¦ä¿¡æ¯ç‚¹ç¼ºå¤±ï¼‰ï¼Œ`sufficient` ä¸º `false`ï¼Œ`missing_points` åªåˆ—å‡º**å…³é”®ç¼ºå¤±çš„ä¸»è¦ä¿¡æ¯ç‚¹**ï¼ˆä¸è¦æ±‚åˆ—å‡ºæ‰€æœ‰ç»†èŠ‚ç¼ºå¤±ï¼‰ã€‚"""

        # æ ¼å¼åŒ–æ£€ç´¢ç»“æœæ‘˜è¦
        results_summary = ""
        if search_results:
            results_summary = "æ£€ç´¢ç»“æœï¼ˆå…±{}æ¡ï¼‰ï¼š\n".format(len(search_results))
            for i, result in enumerate(search_results[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
                title = result.get("title", result.get("filename", "æ— æ ‡é¢˜"))
                content_preview = result.get("content", "")[:150]
                source = result.get("source", result.get("url", "æœªçŸ¥æ¥æº"))
                results_summary += f"{i}. [{source}] {title}\n   {content_preview}...\n"
        else:
            results_summary = "æ— æ£€ç´¢ç»“æœ\n"
        
        # æ ¼å¼åŒ–å†å²ç« èŠ‚æ‘˜è¦
        history_summary = ""
        if history_sections:
            history_summary = "å†å²ç« èŠ‚å†…å®¹ï¼ˆå…±{}ä¸ªï¼‰ï¼š\n".format(len(history_sections))
            for i, hist_content in enumerate(history_sections, 1):
                preview = hist_content[:200] + "..." if len(hist_content) > 200 else hist_content
                history_summary += f"å†å²ç« èŠ‚ {i}ï¼š\n{preview}\n"
        else:
            history_summary = "æ— å†å²ç« èŠ‚\n"
        
        user_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿæ”¯æ’‘ç« èŠ‚å†™ä½œï¼š

ç« èŠ‚ä¿¡æ¯ï¼š
- ä¸€çº§æ ‡é¢˜ï¼š{level1_title}
- äºŒçº§æ ‡é¢˜ï¼š{level2_title}

å®Œæ•´å¤§çº²ï¼š
{outline}

{history_summary}

{results_summary}

**é‡è¦æé†’**ï¼šè¯„ä¼°æ ‡å‡†åº”æ”¾å®½ã€‚åªè¦æ£€ç´¢ç»“æœèƒ½å¤Ÿå¤§ä½“è¦†ç›–ç« èŠ‚ä¸»é¢˜çš„ä¸»è¦æ–¹é¢ï¼Œæœ‰è¶³å¤Ÿä¿¡æ¯æ”¯æ’‘æ’°å†™ä¸€ä¸ªå®Œæ•´ã€æœ‰é€»è¾‘çš„ç« èŠ‚ï¼Œå°±åº”è¯¥åˆ¤æ–­ä¸ºå……è¶³ã€‚ä¸è¦æ±‚æ‰€æœ‰å…·ä½“ç»†èŠ‚ï¼ˆå¦‚å…·ä½“æ—¶é—´ã€é‡‘é¢ã€åç§°ç­‰ï¼‰éƒ½é½å…¨ã€‚

è¯·åˆ¤æ–­ä¿¡æ¯æ˜¯å¦å……è¶³ã€‚å¦‚æœä¸è¶³ï¼Œåªåˆ—å‡ºå…³é”®ç¼ºå¤±çš„ä¸»è¦ä¿¡æ¯ç‚¹ï¼ˆç”¨äºç”Ÿæˆåç»­æ£€ç´¢è¯­å¥ï¼‰ã€‚"""

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"ğŸ“Š [WritingAgent] åˆ¤æ–­ä¿¡æ¯å……è¶³æ€§")
            print(f"{'='*60}")
            print(f"ç« èŠ‚: {level1_title} - {level2_title}")
            print(f"æ£€ç´¢ç»“æœæ•°: {len(search_results)}")
            print(f"å†å²ç« èŠ‚æ•°: {len(history_sections)}")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [WritingAgent] LLM å“åº”å®Œæˆ")
            
            # æå–JSONï¼ˆä½¿ç”¨ç±»ä¼¼ _extract_json çš„é€»è¾‘ï¼‰
            content = self._extract_json(content)
            
            # è§£æJSON
            import json
            evaluation = json.loads(content)
            
            # éªŒè¯å’Œè§„èŒƒåŒ–
            sufficient = bool(evaluation.get("sufficient", False))
            missing_points = evaluation.get("missing_points", [])
            if not isinstance(missing_points, list):
                missing_points = []
            missing_points = [str(p).strip() for p in missing_points if p]
            
            result = {
                "sufficient": sufficient,
                "missing_points": missing_points
            }
            
            print(f"âœ… ä¿¡æ¯å……è¶³æ€§è¯„ä¼°ç»“æœ: å……è¶³={sufficient}, ç¼ºå¤±ç‚¹={missing_points}")
            
            return result
            
        except Exception as e:
            print(f"âš ï¸  ä¿¡æ¯å……è¶³æ€§è¯„ä¼°å¤±è´¥: {e}ï¼Œå‡è®¾ä¿¡æ¯ä¸è¶³")
            # å›é€€ï¼šå‡è®¾ä¿¡æ¯ä¸è¶³
            return {
                "sufficient": False,
                "missing_points": ["ä¿¡æ¯è¯„ä¼°å¤±è´¥ï¼Œå»ºè®®ç»§ç»­æ£€ç´¢"]
            }
    
    def select_history_sections(
        self,
        section: Dict[str, Any],
        max_sections: int = 3
    ) -> tuple[List[str], List[str]]:
        """
        é€‰æ‹©éœ€è¦å›é¡¾çš„å†å²ç« èŠ‚ï¼ˆæœ€å¤š3ä¸ªäºŒçº§æ ‡é¢˜ï¼‰
        
        è¿™ä¸ªæ–¹æ³•ä¼šï¼š
        1. è·å–æ‰€æœ‰å†å²æ ‡é¢˜åˆ—è¡¨
        2. è®©æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦å›é¡¾
        3. è¿”å›é€‰ä¸­çš„å†å²ç« èŠ‚æ ‡é¢˜å’ŒID
        
        Args:
            section: å½“å‰ç« èŠ‚ä¿¡æ¯
            max_sections: æœ€å¤šå›é¡¾çš„ç« èŠ‚æ•°ï¼ˆé»˜è®¤3ï¼‰
        
        Returns:
            (å†å²ç« èŠ‚æ ‡é¢˜åˆ—è¡¨, å†å²ç« èŠ‚IDåˆ—è¡¨)
        """
        if not self.history_manager:
            return [], []
        
        # è·å–æ‰€æœ‰å†å²æ ‡é¢˜
        history_titles = self.history_manager.get_history_titles_formatted()
        
        if not history_titles or history_titles == "æš‚æ— å†å²ç« èŠ‚":
            return [], []
        
        # ä½¿ç”¨LLMåˆ¤æ–­éœ€è¦å›é¡¾å“ªäº›ç« èŠ‚
        system_prompt = """ä½ æ˜¯ä¸€ä½å†…å®¹è¿è´¯æ€§åˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­æ’°å†™å½“å‰ç« èŠ‚æ—¶æ˜¯å¦éœ€è¦å›é¡¾å†å²ç« èŠ‚å†…å®¹ã€‚

è§„åˆ™ï¼š
1. åªèƒ½é€‰æ‹©äºŒçº§æ ‡é¢˜ï¼ˆ### å¼€å¤´çš„ç« èŠ‚ï¼‰
2. æœ€å¤šé€‰æ‹©3ä¸ªç« èŠ‚
3. åªé€‰æ‹©ä¸å½“å‰ç« èŠ‚ç›¸å…³ã€éœ€è¦å‚è€ƒçš„å†å²ç« èŠ‚
4. å¦‚æœä¸éœ€è¦å›é¡¾ä»»ä½•ç« èŠ‚ï¼Œè¿”å›ç©ºåˆ—è¡¨

è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ•°ç»„ï¼ŒåŒ…å«é€‰ä¸­çš„ç« èŠ‚IDï¼š
["section_id1", "section_id2", "section_id3"]

å¦‚æœæ²¡æœ‰éœ€è¦å›é¡¾çš„ç« èŠ‚ï¼Œè¿”å›ç©ºæ•°ç»„ï¼š[]"""

        user_prompt = f"""å½“å‰è¦æ’°å†™çš„ç« èŠ‚ï¼š
- ä¸€çº§æ ‡é¢˜ï¼š{section.get('level1_title', '')}
- äºŒçº§æ ‡é¢˜ï¼š{section.get('level2_title', '')}

å†å²ç« èŠ‚åˆ—è¡¨ï¼š
{history_titles}

è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦å›é¡¾å†å²ç« èŠ‚ï¼Œå¦‚æœéœ€è¦ï¼Œè¯·è¿”å›ç« èŠ‚IDæ•°ç»„ï¼ˆæœ€å¤š3ä¸ªï¼‰ã€‚å¦‚æœä¸éœ€è¦ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚"""
        
        # ç¡®ä¿ prompt ä¸ä¸ºç©º
        if not user_prompt or len(user_prompt.strip()) == 0:
            user_prompt = f"""å½“å‰è¦æ’°å†™çš„ç« èŠ‚ï¼š
- ä¸€çº§æ ‡é¢˜ï¼š{section.get('level1_title', 'ç« èŠ‚')}
- äºŒçº§æ ‡é¢˜ï¼š{section.get('level2_title', 'å†…å®¹')}

æš‚æ— å†å²ç« èŠ‚ã€‚

è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦å›é¡¾å†å²ç« èŠ‚ï¼Œå¦‚æœéœ€è¦ï¼Œè¯·è¿”å›ç« èŠ‚IDæ•°ç»„ï¼ˆæœ€å¤š3ä¸ªï¼‰ã€‚å¦‚æœä¸éœ€è¦ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚"""

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"ğŸ” [WritingAgent] è°ƒç”¨ LLM é€‰æ‹©å†å²ç« èŠ‚")
            print(f"{'='*60}")
            print(f"æ¨¡å‹: qwen-plus")
            print(f"å½“å‰ç« èŠ‚: {section.get('level1_title', '')} - {section.get('level2_title', '')}")
            history_count = len([line for line in history_titles.split('\n') if line.strip()]) if history_titles else 0
            print(f"å†å²ç« èŠ‚æ•°: {history_count}")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [WritingAgent] é€‰æ‹©å†å²ç« èŠ‚å®Œæˆ")
            
            # æå–JSONæ•°ç»„
            import json
            import re
            
            # å°è¯•æå–JSONæ•°ç»„
            content = re.sub(r'[^\[\],"\w]', '', content)  # æ¸…ç†éJSONå­—ç¬¦
            if content.startswith('[') and content.endswith(']'):
                section_ids = json.loads(content)
            else:
                # å°è¯•æ‰¾åˆ°æ•°ç»„
                match = re.search(r'\[(.*?)\]', content)
                if match:
                    section_ids = json.loads(match.group(0))
                else:
                    section_ids = []
            
            # é™åˆ¶æœ€å¤š3ä¸ª
            section_ids = section_ids[:max_sections]
            
            # è·å–ç« èŠ‚æ ‡é¢˜ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
            history_titles = []
            titles_list = self.history_manager.get_history_titles()
            titles_dict = {t['section_id']: t for t in titles_list if t.get('section_id')}
            
            for section_id in section_ids:
                if section_id in titles_dict:
                    title_info = titles_dict[section_id]
                    # è·å–æ ‡é¢˜ï¼Œç§»é™¤ markdown æ ‡è®°
                    title = title_info.get('title', '')
                    # ç§»é™¤å¯èƒ½çš„ markdown æ ‡è®°ï¼ˆ###ã€##ã€# ç­‰ï¼‰
                    title = title.lstrip('#').strip()
                    if title:
                        history_titles.append(title)
            
            # è¿”å›æ ‡é¢˜åˆ—è¡¨å’ŒIDåˆ—è¡¨
            return history_titles, section_ids
            
        except Exception as e:
            print(f"é€‰æ‹©å†å²ç« èŠ‚å¤±è´¥: {e}ï¼Œå°†ä¸å›é¡¾å†å²ç« èŠ‚")
            return [], []

