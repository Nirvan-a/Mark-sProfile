"""
è§„åˆ’æ™ºèƒ½ä½“
æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆå†™ä½œå¤§çº²ï¼ŒåŒ…å«æ€»æ ‡é¢˜ã€ä¸€çº§æ ‡é¢˜ã€äºŒçº§æ ‡é¢˜
"""
import os
import json
import re
from typing import Dict, List, Any, Optional

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain_community.chat_models import ChatOpenAI


class PlanningAgentError(Exception):
    """è§„åˆ’æ™ºèƒ½ä½“é”™è¯¯"""
    pass


class PlanningAgent:
    """è§„åˆ’æ™ºèƒ½ä½“ - ç”Ÿæˆå†™ä½œå¤§çº²"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§„åˆ’æ™ºèƒ½ä½“"""
        # ç¡®ä¿åŠ è½½ .env æ–‡ä»¶
        from dotenv import load_dotenv
        from pathlib import Path
        env_path = Path(__file__).parent.parent.parent.parent / ".env"
        load_dotenv(dotenv_path=env_path, override=False)
        
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise PlanningAgentError("DASHSCOPE_API_KEY æœªé…ç½®")
        
        self.llm = ChatOpenAI(
            model="qwen-plus",
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            temperature=0.7,
        )
    
    def generate_outline(self, requirement: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆå†™ä½œå¤§çº²
        
        Args:
            requirement: ç”¨æˆ·éœ€æ±‚/é—®é¢˜
        
        Returns:
            å¤§çº²å­—å…¸ï¼ŒåŒ…å«:
            {
                "title": "æ€»æ ‡é¢˜",
                "sections": [
                    {
                        "level1_title": "ä¸€çº§æ ‡é¢˜",
                        "level2_titles": ["äºŒçº§æ ‡é¢˜1", "äºŒçº§æ ‡é¢˜2", ...]
                    },
                    ...
                ],
                "estimated_words": é¢„ä¼°å­—æ•°,
                "outline_markdown": "Markdownæ ¼å¼çš„å¤§çº²"
            }
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ¥å‘Šè§„åˆ’ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†çš„å†™ä½œå¤§çº²ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ç”Ÿæˆå¤§çº²ï¼š
1. å¿…é¡»åŒ…å«ä¸€ä¸ªæ€»æ ‡é¢˜ï¼ˆä¸€çº§æ ‡é¢˜ #ï¼‰
2. åŒ…å«2-3ä¸ªä¸€çº§æ ‡é¢˜ï¼ˆäºŒçº§æ ‡é¢˜ ##ï¼‰
3. æ¯ä¸ªä¸€çº§æ ‡é¢˜ä¸‹å¯ä»¥åŒ…å«2-4ä¸ªäºŒçº§æ ‡é¢˜ï¼ˆä¸‰çº§æ ‡é¢˜ ###ï¼‰ï¼Œä¹Ÿå¯ä»¥ä¸åŒ…å«äºŒçº§æ ‡é¢˜
4. å¦‚æœç« èŠ‚å†…å®¹ç®€å•ï¼Œå¯ä»¥ä¸è®¾ç½®äºŒçº§æ ‡é¢˜ï¼Œç›´æ¥åœ¨ä¸€çº§æ ‡é¢˜ä¸‹æ’°å†™
5. ä½¿ç”¨Markdownæ ¼å¼è¾“å‡º
6. å¤§çº²åº”è¯¥ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†
7. **å­—æ•°è¦æ±‚**ï¼š
   - ä»”ç»†åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ç›®æ ‡å­—æ•°ï¼ˆå¦‚"å†™ä¸€ç¯‡5000å­—çš„æ–‡ç« "ã€"å¤§çº¦3000å­—"ç­‰ï¼‰
   - å¦‚æœç”¨æˆ·æ˜ç¡®æåˆ°äº†å­—æ•°ï¼Œä½¿ç”¨ç”¨æˆ·æåˆ°çš„å­—æ•°ä½œä¸º estimated_words
   - å¦‚æœç”¨æˆ·æ²¡æœ‰æåˆ°å­—æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1500 ä½œä¸º estimated_words

è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- title: æ€»æ ‡é¢˜ï¼ˆå­—ç¬¦ä¸²ï¼‰
- sections: ä¸€çº§æ ‡é¢˜æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
  - level1_title: ä¸€çº§æ ‡é¢˜ï¼ˆå­—ç¬¦ä¸²ï¼‰
  - level2_titles: äºŒçº§æ ‡é¢˜æ•°ç»„ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œå¯ä»¥ä¸ºç©ºæ•°ç»„ []ï¼Œæˆ–è€…åŒ…å«2-4ä¸ªäºŒçº§æ ‡é¢˜ï¼‰
- estimated_words: é¢„ä¼°æ€»å­—æ•°ï¼ˆæ•´æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æåˆ°çš„å­—æ•°ï¼Œå¦åˆ™ä½¿ç”¨1500ï¼‰
- outline_markdown: Markdownæ ¼å¼çš„å®Œæ•´å¤§çº²ï¼ˆå­—ç¬¦ä¸²ï¼‰

ç¤ºä¾‹JSONæ ¼å¼ï¼š
{
  "title": "æ ¸èƒ½å‘å±•ç°çŠ¶ä¸æœªæ¥å±•æœ›",
  "sections": [
    {
      "level1_title": "æ ¸èƒ½å‘å±•ç°çŠ¶",
      "level2_titles": [
        "å…¨çƒæ ¸èƒ½è£…æœºå®¹é‡åˆ†æ",
        "ä¸»è¦å›½å®¶æ ¸èƒ½æ”¿ç­–å¯¹æ¯”",
        "æ ¸èƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿"
      ]
    },
    {
      "level1_title": "æ ¸èƒ½å‘å±•é¢ä¸´çš„æŒ‘æˆ˜",
      "level2_titles": [
        "å®‰å…¨æ€§é—®é¢˜",
        "æ ¸åºŸæ–™å¤„ç†",
        "å…¬ä¼—æ¥å—åº¦"
      ]
    },
    {
      "level1_title": "æ ¸èƒ½æœªæ¥å‘å±•å±•æœ›",
      "level2_titles": [
        "å°å‹æ¨¡å—åŒ–ååº”å †",
        "æ ¸èšå˜æŠ€æœ¯",
        "æ ¸èƒ½ä¸å…¶ä»–èƒ½æºçš„ååŒ"
      ]
    }
  ],
  "estimated_words": 8000,
  "outline_markdown": "# æ ¸èƒ½å‘å±•ç°çŠ¶ä¸æœªæ¥å±•æœ›\\n\\n## æ ¸èƒ½å‘å±•ç°çŠ¶\\n\\n### å…¨çƒæ ¸èƒ½è£…æœºå®¹é‡åˆ†æ\\n### ä¸»è¦å›½å®¶æ ¸èƒ½æ”¿ç­–å¯¹æ¯”\\n### æ ¸èƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿\\n\\n## æ ¸èƒ½å‘å±•é¢ä¸´çš„æŒ‘æˆ˜\\n\\n### å®‰å…¨æ€§é—®é¢˜\\n### æ ¸åºŸæ–™å¤„ç†\\n### å…¬ä¼—æ¥å—åº¦\\n\\n## æ ¸èƒ½æœªæ¥å‘å±•å±•æœ›\\n\\n### å°å‹æ¨¡å—åŒ–ååº”å †\\n### æ ¸èšå˜æŠ€æœ¯\\n### æ ¸èƒ½ä¸å…¶ä»–èƒ½æºçš„ååŒ"
}

è¯·ç¡®ä¿ï¼š
- JSONæ ¼å¼ä¸¥æ ¼æ­£ç¡®ï¼Œå¯ä»¥è¢«Pythonçš„json.loads()è§£æ
- æ€»æ ‡é¢˜å’Œæ‰€æœ‰ä¸€çº§ã€äºŒçº§æ ‡é¢˜éƒ½æœ‰æ„ä¹‰ä¸”ç›¸å…³
- äºŒçº§æ ‡é¢˜æ•°é‡åˆç†ï¼ˆæ¯ä¸ªä¸€çº§æ ‡é¢˜ä¸‹2-3ä¸ªï¼‰
"""

        user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„å†™ä½œå¤§çº²ï¼š

ç”¨æˆ·éœ€æ±‚ï¼š{requirement}

è¯·ä»”ç»†åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ç›®æ ‡å­—æ•°ã€‚å¦‚æœç”¨æˆ·æåˆ°äº†å­—æ•°ï¼ˆå¦‚"å†™ä¸€ç¯‡5000å­—çš„æ–‡ç« "ã€"å¤§çº¦3000å­—"ã€"æ§åˆ¶åœ¨2000å­—ä»¥å†…"ç­‰ï¼‰ï¼Œä½¿ç”¨è¯¥å­—æ•°ä½œä¸º estimated_wordsï¼›å¦‚æœæ²¡æœ‰æåˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1500ã€‚

è¯·ç”ŸæˆåŒ…å«æ€»æ ‡é¢˜ã€2-3ä¸ªä¸€çº§æ ‡é¢˜ã€æ¯ä¸ªä¸€çº§æ ‡é¢˜ä¸‹2-3ä¸ªäºŒçº§æ ‡é¢˜çš„JSONæ ¼å¼å¤§çº²ã€‚"""

        try:
            # è°ƒç”¨ LLM
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            
            print(f"\n{'='*60}")
            print(f"ğŸ¤– [PlanningAgent] è°ƒç”¨ LLM ç”Ÿæˆå¤§çº²")
            print(f"{'='*60}")
            print(f"æ¨¡å‹: qwen-plus")
            print(f"ç”¨æˆ·éœ€æ±‚: {requirement[:100]}...")
            print(f"å¼€å§‹è¯·æ±‚...")
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            print(f"âœ… [PlanningAgent] LLM å“åº”å®Œæˆ (é•¿åº¦: {len(content)} å­—ç¬¦)")
            
            # å°è¯•æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            content = self._extract_json(content)
            
            # è§£æJSON
            outline_data = json.loads(content)
            
            # éªŒè¯å’Œè§„èŒƒåŒ–
            outline_data = self._validate_and_normalize(outline_data)
            
            # ç”ŸæˆMarkdownæ ¼å¼çš„å¤§çº²ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if "outline_markdown" not in outline_data or not outline_data["outline_markdown"]:
                outline_data["outline_markdown"] = self._generate_markdown_outline(outline_data)
            
            return outline_data
            
        except json.JSONDecodeError as e:
            raise PlanningAgentError(f"è§£æå¤§çº²JSONå¤±è´¥: {str(e)}") from e
        except Exception as e:
            error_msg = str(e)
            if "API key" in error_msg or "401" in error_msg or "authentication" in error_msg.lower():
                raise PlanningAgentError("API Key è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ DASHSCOPE_API_KEY é…ç½®") from e
            raise PlanningAgentError(f"ç”Ÿæˆå¤§çº²å¤±è´¥: {error_msg}") from e
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰"""
        text = text.strip()
        
        # å¦‚æœåŒ…å«markdownä»£ç å—
        if text.startswith("```"):
            lines = text.split("\n")
            json_start = 0
            json_end = len(lines)
            
            for i, line in enumerate(lines):
                if line.strip().startswith("```"):
                    if json_start == 0:
                        json_start = i + 1
                    else:
                        json_end = i
                        break
            
            text = "\n".join(lines[json_start:json_end])
        
        # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start_idx = text.find("{")
        end_idx = text.rfind("}")
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            text = text[start_idx:end_idx + 1]
        
        return text.strip()
    
    def _validate_and_normalize(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œè§„èŒƒåŒ–å¤§çº²æ•°æ®"""
        # å¿…éœ€å­—æ®µ
        if "title" not in data:
            raise PlanningAgentError("å¤§çº²ç¼ºå°‘æ€»æ ‡é¢˜")
        if "sections" not in data:
            raise PlanningAgentError("å¤§çº²ç¼ºå°‘ç« èŠ‚åˆ—è¡¨")
        
        # éªŒè¯sections
        if not isinstance(data["sections"], list):
            raise PlanningAgentError("sectionså¿…é¡»æ˜¯æ•°ç»„")
        
        if len(data["sections"]) < 2 or len(data["sections"]) > 4:
            raise PlanningAgentError("ä¸€çº§æ ‡é¢˜æ•°é‡åº”è¯¥åœ¨2-4ä¸ªä¹‹é—´")
        
        # è§„èŒƒåŒ–æ¯ä¸ªsection
        normalized_sections = []
        for i, section in enumerate(data["sections"]):
            if not isinstance(section, dict):
                raise PlanningAgentError(f"ç¬¬{i+1}ä¸ªsectionæ ¼å¼ä¸æ­£ç¡®")
            
            if "level1_title" not in section:
                raise PlanningAgentError(f"ç¬¬{i+1}ä¸ªsectionç¼ºå°‘ä¸€çº§æ ‡é¢˜")
            
            if "level2_titles" not in section:
                raise PlanningAgentError(f"ç¬¬{i+1}ä¸ªsectionç¼ºå°‘äºŒçº§æ ‡é¢˜åˆ—è¡¨")
            
            level2_titles = section["level2_titles"]
            if not isinstance(level2_titles, list):
                raise PlanningAgentError(f"ç¬¬{i+1}ä¸ªsectionçš„äºŒçº§æ ‡é¢˜å¿…é¡»æ˜¯æ•°ç»„")
            
            # å…è®¸äºŒçº§æ ‡é¢˜ä¸ºç©ºï¼ˆ0ä¸ªï¼‰ï¼Œæˆ–è€…2-4ä¸ª
            if len(level2_titles) > 0 and (len(level2_titles) < 2 or len(level2_titles) > 4):
                raise PlanningAgentError(f"ç¬¬{i+1}ä¸ªsectionçš„äºŒçº§æ ‡é¢˜æ•°é‡åº”è¯¥åœ¨2-4ä¸ªä¹‹é—´ï¼Œæˆ–è€…ä¸ºç©ºï¼ˆä¸éœ€è¦äºŒçº§æ ‡é¢˜ï¼‰")
            
            normalized_sections.append({
                "level1_title": str(section["level1_title"]).strip(),
                "level2_titles": [str(t).strip() for t in level2_titles],
            })
        
        # ä¼°ç®—å­—æ•°ï¼ˆå¦‚æœLLMæ²¡æœ‰ç”Ÿæˆï¼Œä½¿ç”¨é»˜è®¤å€¼1500ï¼‰
        if "estimated_words" not in data or not data.get("estimated_words"):
            data["estimated_words"] = 1500  # é»˜è®¤1500å­—
        
        return {
            "title": str(data["title"]).strip(),
            "sections": normalized_sections,
            "estimated_words": int(data.get("estimated_words", 0)),
            "outline_markdown": data.get("outline_markdown", ""),
        }
    
    def _generate_markdown_outline(self, data: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„å¤§çº²"""
        lines = [f"# {data['title']}", ""]
        
        for section in data["sections"]:
            lines.append(f"## {section['level1_title']}")
            lines.append("")
            for level2_title in section["level2_titles"]:
                lines.append(f"### {level2_title}")
            lines.append("")
        
        return "\n".join(lines)
    
    def get_all_level2_sections(self, outline: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰äºŒçº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨ï¼ˆç”¨äºå†™ä½œå¾ªç¯ï¼‰
        
        Args:
            outline: å¤§çº²å­—å…¸
        
        Returns:
            äºŒçº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            {
                "section_id": ç« èŠ‚ID,
                "level1_title": ä¸€çº§æ ‡é¢˜,
                "level2_title": äºŒçº§æ ‡é¢˜,
                "index": ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            }
        """
        sections = []
        index = 1
        
        for section in outline["sections"]:
            level1_title = section["level1_title"]
            for level2_title in section["level2_titles"]:
                sections.append({
                    "section_id": f"section_{index}",
                    "level1_title": level1_title,
                    "level2_title": level2_title,
                    "index": index,
                })
                index += 1
        
        return sections
    
    def get_all_level1_sections(self, outline: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰ä¸€çº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨ï¼ˆç”¨äºæ–°çš„å†™ä½œå¾ªç¯ï¼ŒæŒ‰ä¸€çº§æ ‡é¢˜ä¸ºå•ä½ï¼‰
        
        éå†å¤§çº²çš„æ‰€æœ‰ç« èŠ‚ï¼Œä¸ºæ¯ä¸ªä¸€çº§æ ‡é¢˜åˆ›å»ºä¸€ä¸ª section å¯¹è±¡
        
        Args:
            outline: å¤§çº²å­—å…¸
        
        Returns:
            ä¸€çº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            {
                "section_id": ç« èŠ‚ID,
                "level1_title": ä¸€çº§æ ‡é¢˜,
                "level2_titles": äºŒçº§æ ‡é¢˜åˆ—è¡¨,
                "index": ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            }
        """
        sections = []
        
        for index, section in enumerate(outline["sections"], 1):
            sections.append({
                "section_id": f"section_{index}",
                "level1_title": section["level1_title"],
                "level2_titles": section["level2_titles"],
                "index": index,
            })
        
        return sections

    def parse_markdown_outline(self, markdown_text: str) -> Dict[str, Any]:
        """
        ä» Markdown æ–‡æœ¬è§£æå¤§çº²ç»“æ„
        
        Args:
            markdown_text: Markdown æ ¼å¼çš„å¤§çº²æ–‡æœ¬
            
        Returns:
            å¤§çº²å­—å…¸ï¼ŒåŒ…å«:
            {
                "title": "æ€»æ ‡é¢˜",
                "sections": [
                    {
                        "level1_title": "ä¸€çº§æ ‡é¢˜",
                        "level2_titles": ["äºŒçº§æ ‡é¢˜1", "äºŒçº§æ ‡é¢˜2", ...]
                    },
                    ...
                ],
                "estimated_words": é¢„ä¼°å­—æ•°,
                "outline_markdown": markdown_text
            }
        """
        lines = markdown_text.strip().split('\n')
        title = ""
        sections = []
        current_level1 = None
        current_level2_titles = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è§£ææ€»æ ‡é¢˜ï¼ˆ# å¼€å¤´ï¼‰
            if line.startswith('# ') and not line.startswith('##'):
                title = line[2:].strip()
            # è§£æä¸€çº§æ ‡é¢˜ï¼ˆ## å¼€å¤´ï¼‰
            elif line.startswith('## ') and not line.startswith('###'):
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_level1 is not None:
                    sections.append({
                        "level1_title": current_level1,
                        "level2_titles": current_level2_titles
                    })
                # å¼€å§‹æ–°çš„ä¸€çº§æ ‡é¢˜
                current_level1 = line[3:].strip()
                current_level2_titles = []
            # è§£æäºŒçº§æ ‡é¢˜ï¼ˆ### å¼€å¤´ï¼‰
            elif line.startswith('### '):
                level2_title = line[4:].strip()
                if level2_title:
                    current_level2_titles.append(level2_title)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_level1 is not None:
            sections.append({
                "level1_title": current_level1,
                "level2_titles": current_level2_titles
            })
        
        # å¦‚æœä» markdown è§£æï¼Œä½¿ç”¨é»˜è®¤å€¼1500å­—ï¼ˆå› ä¸ºæ— æ³•ä» markdown ä¸­è·å–ç”¨æˆ·æåˆ°çš„å­—æ•°ï¼‰
        estimated_words = 1500
        
        return {
            "title": title,
            "sections": sections,
            "estimated_words": estimated_words,
            "outline_markdown": markdown_text
        }

