"""
Deep Research å·¥ä½œæµç¼–æ’å™¨
ä½¿ç”¨ LangGraph ç¼–æ’å®Œæ•´çš„æ·±åº¦ç ”ç©¶å†™ä½œæµç¨‹
"""
import os
from typing import TypedDict, List, Dict, Any, Optional
from uuid import uuid4

try:
    from langgraph.graph import StateGraph, END
    from langgraph.graph.message import add_messages
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("è­¦å‘Š: LangGraph ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install langgraph")

from ..agents.planning_agent import PlanningAgent, PlanningAgentError
from ..tools.temporary_kb import TemporaryKnowledgeBase
from ..tools.writing_history import WritingHistoryManager
from ..tools.tool_orchestrator import ToolOrchestrator
from ..agents.writing_agent import WritingAgent
from ..agents.result_filter_agent import ResultFilterAgent


class WorkflowError(Exception):
    """å·¥ä½œæµé”™è¯¯"""
    pass


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    # è¾“å…¥
    requirement: str  # ç”¨æˆ·éœ€æ±‚
    task_id: str  # ä»»åŠ¡ID
    
    # è§„åˆ’é˜¶æ®µ
    outline: Optional[Dict[str, Any]]  # å¤§çº²
    sections: List[Dict[str, Any]]  # æ‰€æœ‰äºŒçº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨
    current_section_index: int  # å½“å‰ç« èŠ‚ç´¢å¼•
    
    # å†™ä½œé˜¶æ®µ
    current_section: Optional[Dict[str, Any]]  # å½“å‰ç« èŠ‚ä¿¡æ¯
    search_results: List[Dict[str, Any]]  # å½“å‰ç« èŠ‚çš„æœ€ç»ˆæ£€ç´¢ç»“æœï¼ˆç”¨äºå†™ä½œï¼‰
    history_sections: List[str]  # éœ€è¦å›é¡¾çš„å†å²ç« èŠ‚æ ‡é¢˜ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
    history_section_ids: List[str]  # éœ€è¦å›é¡¾çš„å†å²ç« èŠ‚IDï¼ˆç”¨äºè·å–å†…å®¹ï¼‰
    written_content: str  # å½“å‰ç« èŠ‚çš„å†™ä½œå†…å®¹
    written_citations: List[Dict[str, Any]]  # å½“å‰ç« èŠ‚å®é™…ä½¿ç”¨çš„å¼•ç”¨åˆ—è¡¨
    chart_requirement: Optional[Dict[str, Any]]  # å½“å‰ç« èŠ‚çš„å›¾è¡¨éœ€æ±‚
    
    # æ–°æµç¨‹çš„çŠ¶æ€å­—æ®µ
    initial_search_queries: List[str]  # å‡†å¤‡é˜¶æ®µç”Ÿæˆçš„åˆå§‹æ£€ç´¢è¯­å¥
    initial_temp_kb_results: List[Dict[str, Any]]  # ä¸´æ—¶çŸ¥è¯†åº“çš„åˆå§‹æ£€ç´¢ç»“æœ
    info_sufficiency_evaluation: Optional[Dict[str, Any]]  # ä¿¡æ¯å……è¶³æ€§åˆ¤æ–­ç»“æœ
    additional_search_queries: List[str]  # å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œç”Ÿæˆçš„é¢å¤–æ£€ç´¢è¯­å¥
    additional_search_results: List[Dict[str, Any]]  # é¢å¤–æ£€ç´¢çš„ç»“æœï¼ˆç­›é€‰å‰ï¼‰
    filtered_results: List[Dict[str, Any]]  # ç­›é€‰åçš„ç»“æœ
    
    # å®ŒæˆçŠ¶æ€
    all_written_sections: List[Dict[str, Any]]  # æ‰€æœ‰å·²å†™ç« èŠ‚
    is_complete: bool  # æ˜¯å¦å®Œæˆ
    
    # ç»„ä»¶å®ä¾‹ï¼ˆä¸åºåˆ—åŒ–ï¼Œè¿è¡Œæ—¶ä½¿ç”¨ï¼‰
    temp_kb: Optional[TemporaryKnowledgeBase]
    history_manager: Optional[WritingHistoryManager]
    tool_orchestrator: Optional[ToolOrchestrator]
    writing_agent: Optional[WritingAgent]


def create_deep_research_workflow():
    """åˆ›å»º Deep Research å·¥ä½œæµ"""
    if not LANGGRAPH_AVAILABLE:
        raise WorkflowError("LangGraph ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install langgraph")
    
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("initialize", initialize_node)
    workflow.add_node("planning", planning_node)
    workflow.add_node("prepare_section", prepare_section_node)
    workflow.add_node("collect_info", collect_info_node)
    workflow.add_node("writing", writing_node)
    workflow.add_node("save_section", save_section_node)
    workflow.add_node("complete", complete_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("initialize")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("initialize", "planning")
    workflow.add_edge("planning", "prepare_section")
    
    # æ¡ä»¶è¾¹ï¼šæ˜¯å¦è¿˜æœ‰ç« èŠ‚
    workflow.add_conditional_edges(
        "prepare_section",
        has_more_sections,
        {
            "yes": "collect_info",
            "no": "complete",
        }
    )
    
    workflow.add_edge("collect_info", "writing")
    workflow.add_edge("writing", "save_section")
    
    # æ¡ä»¶è¾¹ï¼šæ˜¯å¦è¿˜æœ‰ç« èŠ‚
    workflow.add_conditional_edges(
        "save_section",
        has_more_sections,
        {
            "yes": "prepare_section",
            "no": "complete",
        }
    )
    
    workflow.add_edge("complete", END)
    
    return workflow.compile()


def initialize_node(state: WorkflowState) -> WorkflowState:
    """åˆå§‹åŒ–èŠ‚ç‚¹"""
    print("=" * 50)
    print("åˆå§‹åŒ– Deep Research å·¥ä½œæµ")
    print("=" * 50)
    
    # ç”Ÿæˆä»»åŠ¡ID
    if not state.get("task_id"):
        state["task_id"] = uuid4().hex
    
    # åˆå§‹åŒ–ç»„ä»¶
    state["temp_kb"] = TemporaryKnowledgeBase(task_id=state["task_id"])
    state["history_manager"] = WritingHistoryManager()
    state["tool_orchestrator"] = ToolOrchestrator(state["temp_kb"])
    state["writing_agent"] = WritingAgent(history_manager=state["history_manager"])
    
    # åˆå§‹åŒ–çŠ¶æ€ï¼ˆå¦‚æœ sections å·²å­˜åœ¨ï¼Œä¸è¦æ¸…ç©ºï¼Œå› ä¸ºå¯èƒ½æ˜¯ä»å‰ç«¯ä¼ é€’è¿‡æ¥çš„ï¼‰
    existing_sections = state.get("sections", [])
    if not existing_sections:
        state["sections"] = []
    else:
        print(f"[Initialize] ä¿ç•™å·²å­˜åœ¨çš„ sectionsï¼Œæ•°é‡: {len(existing_sections)}")
    if "current_section_index" not in state:
        state["current_section_index"] = 0
    if "all_written_sections" not in state:
        state["all_written_sections"] = []
    if "is_complete" not in state:
        state["is_complete"] = False
    
    print(f"ä»»åŠ¡ID: {state['task_id']}")
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    return state


def planning_node(state: WorkflowState) -> WorkflowState:
    """è§„åˆ’èŠ‚ç‚¹ - ç”Ÿæˆå¤§çº²ï¼ˆå¦‚æœå·²æœ‰å¤§çº²åˆ™è·³è¿‡ç”Ÿæˆï¼‰"""
    print("\n" + "=" * 50)
    print("è§„åˆ’é˜¶æ®µï¼šç”Ÿæˆå†™ä½œå¤§çº²")
    print("=" * 50)
    
    # å¦‚æœå·²æœ‰å¤§çº²ï¼ˆä»å‰ç«¯ä¼ é€’è¿‡æ¥çš„ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸é‡æ–°ç”Ÿæˆ
    existing_outline = state.get("outline")
    existing_sections = state.get("sections", [])
    
    if existing_outline and existing_sections:
        print(f"âœ… ä½¿ç”¨å·²æä¾›çš„å¤§çº²ï¼ˆè·³è¿‡ç”Ÿæˆï¼‰")
        print(f"  æ€»æ ‡é¢˜: {existing_outline.get('title', '')}")
        print(f"  ç« èŠ‚æ•°ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰: {len(existing_sections)}")
        print(f"  é¢„ä¼°å­—æ•°: {existing_outline.get('estimated_words', 0)}")
        state["current_section_index"] = 0
        return state
    
    # å¦‚æœæ²¡æœ‰å¤§çº²ï¼Œåˆ™ç”Ÿæˆæ–°çš„å¤§çº²
    requirement = state.get("requirement", "")
    if not requirement:
        raise WorkflowError("ç”¨æˆ·éœ€æ±‚ä¸èƒ½ä¸ºç©º")
    
    try:
        planner = PlanningAgent()
        outline = planner.generate_outline(requirement)
        
        # è·å–æ‰€æœ‰ä¸€çº§æ ‡é¢˜ç« èŠ‚åˆ—è¡¨ï¼ˆæ”¹ä¸ºæŒ‰ä¸€çº§æ ‡é¢˜ä¸ºå•ä½ï¼‰
        sections = planner.get_all_level1_sections(outline)
        
        state["outline"] = outline
        state["sections"] = sections
        state["current_section_index"] = 0
        
        print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ")
        print(f"  æ€»æ ‡é¢˜: {outline['title']}")
        print(f"  ç« èŠ‚æ•°ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰: {len(sections)}")
        print(f"  é¢„ä¼°å­—æ•°: {outline.get('estimated_words', 0)}")
        
    except PlanningAgentError as e:
        raise WorkflowError(f"è§„åˆ’å¤±è´¥: {str(e)}") from e
    
    return state


def prepare_section_node(state: WorkflowState) -> WorkflowState:
    """å‡†å¤‡ç« èŠ‚èŠ‚ç‚¹ - å‡†å¤‡å½“å‰ç« èŠ‚ä¿¡æ¯"""
    import time
    
    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç« èŠ‚
    if current_index >= len(sections):
        state["is_complete"] = True
        state["current_section"] = None
        return state
    
    current_section = sections[current_index]
    state["current_section"] = current_section
    state["search_results"] = []
    state["history_sections"] = []
    state["history_section_ids"] = []
    state["written_content"] = ""
    
    # åˆå§‹åŒ–æ–°æµç¨‹çš„çŠ¶æ€å­—æ®µ
    state["initial_search_queries"] = []
    state["initial_temp_kb_results"] = []
    state["info_sufficiency_evaluation"] = None
    state["additional_search_queries"] = []
    state["additional_search_results"] = []
    state["filtered_results"] = []
    
    print("\n" + "=" * 50)
    print(f"å‡†å¤‡ç« èŠ‚ {current_index + 1}/{len(sections)}")
    print(f"  ä¸€çº§æ ‡é¢˜: {current_section.get('level1_title', '')}")
    print(f"  äºŒçº§æ ‡é¢˜: {current_section.get('level2_title', '')}")
    print("=" * 50)
    
    outline = state.get("outline", {})
    requirement = state.get("requirement", "")
    writing_agent = state.get("writing_agent")
    temp_kb = state.get("temp_kb")
    history_manager = state.get("history_manager")
    
    # ä½¿ç”¨å…¨å±€è¿›åº¦ç®¡ç†å™¨
    from ..tools.progress_manager import get_progress_manager
    progress_manager = get_progress_manager()
    task_id = state.get("task_id")
    
    def report_progress(step: int, total: int, message: str, data: dict = None):
        """æŠ¥å‘Šæ­¥éª¤è¿›åº¦åˆ°å‰ç«¯"""
        if task_id:
            event_data = {
                "type": "step_progress",
                "node": "prepare_section",
                "step": step,
                "total": total,
                "message": message,
                "timestamp": int(time.time() * 1000)
            }
            if data:
                event_data["data"] = data
            progress_manager.report_progress(task_id, event_data)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    step_start_time = time.time()
    
    # 1. é€‰æ‹©éœ€è¦å›é¡¾çš„å†å²ç« èŠ‚
    if history_manager:
        history_titles = history_manager.get_history_titles_formatted()
        print(f"\nå†å²ç« èŠ‚åˆ—è¡¨:\n{history_titles}\n")
        
        if writing_agent:
            report_progress(1, 6, "ğŸ” åˆ¤æ–­æ˜¯å¦éœ€è¦å›é¡¾å†å²ç« èŠ‚...")
            print(f"â±ï¸  [æ­¥éª¤1å¼€å§‹] é€‰æ‹©å†å²ç« èŠ‚ - {time.strftime('%H:%M:%S')}")
            history_titles, history_ids = writing_agent.select_history_sections(
                current_section,
                max_sections=3
            )
            state["history_sections"] = history_titles  # æ ‡é¢˜åˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
            state["history_section_ids"] = history_ids  # IDåˆ—è¡¨ï¼ˆç”¨äºè·å–å†…å®¹ï¼‰
            
            elapsed = time.time() - step_start_time
            if history_titles:
                print(f"âœ… [æ­¥éª¤1å®Œæˆ] é€‰æ‹©å›é¡¾ {len(history_titles)} ä¸ªå†å²ç« èŠ‚ - è€—æ—¶ {elapsed:.2f}ç§’")
                report_progress(1, 6, f"âœ… éœ€è¦å›é¡¾ {len(history_titles)} ä¸ªå†å²ç« èŠ‚", {
                    "history_sections": history_titles
                })
            else:
                print(f"âœ… [æ­¥éª¤1å®Œæˆ] æ— éœ€å›é¡¾å†å²ç« èŠ‚ - è€—æ—¶ {elapsed:.2f}ç§’")
                report_progress(1, 6, "âœ… æ— éœ€å›é¡¾å†å²ç« èŠ‚")
    
    # 2. ç”Ÿæˆæ£€ç´¢è¯­å¥å¹¶å¹¶è¡Œæ£€ç´¢ã€ç­›é€‰ã€å…¥åº“
    if writing_agent and outline and temp_kb:
        step_start_time = time.time()
        report_progress(2, 6, "ğŸ” ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢...")
        print(f"\nâ±ï¸  [æ­¥éª¤2å¼€å§‹] ç”Ÿæˆæ£€ç´¢æŸ¥è¯¢è¯­å¥ - {time.strftime('%H:%M:%S')}")
        search_queries = writing_agent.generate_search_queries(
            section=current_section,
            outline=outline,
            requirement=requirement
        )
        state["initial_search_queries"] = search_queries
        elapsed = time.time() - step_start_time
        print(f"âœ… [æ­¥éª¤2å®Œæˆ] ç”Ÿæˆ {len(search_queries)} ä¸ªæ£€ç´¢æŸ¥è¯¢ - è€—æ—¶ {elapsed:.2f}ç§’")
        report_progress(2, 6, f"å·²ç”Ÿæˆ {len(search_queries)} ä¸ªæ£€ç´¢æŸ¥è¯¢", {
            "search_queries": search_queries
        })
        
        # æ­¥éª¤3: å…ˆåœ¨ä¸´æ—¶çŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ¯ä¸ªæŸ¥è¯¢åªå–ç¬¬1ä¸ªç»“æœï¼‰
        step_start_time = time.time()
        report_progress(3, 6, "ğŸ” ä¸´æ—¶çŸ¥è¯†åº“æ£€ç´¢...")
        print(f"\nâ±ï¸  [æ­¥éª¤3å¼€å§‹] ä¸´æ—¶çŸ¥è¯†åº“æ£€ç´¢ - {time.strftime('%H:%M:%S')}")
        temp_kb_results = []
        for query in search_queries:
            results = temp_kb.search(query, k=1)  # åªå–ç¬¬1ä¸ªç»“æœ
            if results:
                result = results[0].copy()
                result["query"] = query  # è®°å½•ä½¿ç”¨çš„æŸ¥è¯¢è¯­å¥
                temp_kb_results.append(result)
        elapsed = time.time() - step_start_time
        print(f"âœ… [æ­¥éª¤3å®Œæˆ] ä¸´æ—¶çŸ¥è¯†åº“æ‰¾åˆ° {len(temp_kb_results)} æ¡ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
        report_progress(3, 6, f"ä¸´æ—¶çŸ¥è¯†åº“æ‰¾åˆ° {len(temp_kb_results)} æ¡ç»“æœ")
        
        # æ­¥éª¤4: å¹¶è¡Œæ£€ç´¢çŸ¥è¯†åº“å’Œè”ç½‘ï¼ˆæ¯ä¸ªæŸ¥è¯¢è¯­å¥å–å‰2ç»“æœï¼‰
        from ..services.knowledge_base import get_knowledge_base_manager
        from ..services.web_search import get_web_search_manager
        from ..agents.result_filter_agent import ResultFilterAgent
        import hashlib
        
        main_kb = get_knowledge_base_manager()
        web_search = get_web_search_manager()
        filter_agent = ResultFilterAgent()
        
        # è·å–å·²å…¥åº“çš„ç»“æœIDï¼ˆç”¨äºå»é‡ï¼‰
        def get_result_id(result):
            content = result.get("content", "")
            source = result.get("source", "")
            id_str = f"{content}|{source}"
            return hashlib.md5(id_str.encode()).hexdigest()
        
        existing_ids = set()
        for result in temp_kb_results:
            existing_ids.add(get_result_id(result))
        
        all_initial_results = []
        
        step_start_time = time.time()
        report_progress(4, 6, "ğŸ” å¹¶è¡Œæ£€ç´¢ï¼ˆçŸ¥è¯†åº“ + è”ç½‘ï¼‰...")
        print(f"\nâ±ï¸  [æ­¥éª¤4å¼€å§‹] å¹¶è¡Œæ£€ç´¢ï¼ˆçŸ¥è¯†åº“ + è”ç½‘ï¼‰- {time.strftime('%H:%M:%S')}")
        for query in search_queries:
            # æ£€ç´¢çŸ¥è¯†åº“ï¼ˆå–å‰3ç»“æœï¼Œè¿‡æ»¤å·²å…¥åº“çš„ï¼‰
            try:
                kb_results = main_kb.search(query, k=5)  # å¤šå–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤
                kb_filtered = []
                for result in kb_results:
                    result_id = get_result_id(result)
                    if result_id not in existing_ids:
                        kb_filtered.append(result)
                        existing_ids.add(result_id)
                    if len(kb_filtered) >= 3:  # æ¯ä¸ªæŸ¥è¯¢å–3ä¸ªç»“æœ
                        break
                all_initial_results.extend(kb_filtered)
                if kb_filtered:
                    print(f"  æŸ¥è¯¢ '{query}': çŸ¥è¯†åº“æ‰¾åˆ° {len(kb_filtered)} æ¡ç»“æœ")
            except Exception as e:
                print(f"  æŸ¥è¯¢ '{query}': çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
            
            # è”ç½‘æ£€ç´¢ï¼ˆå–å‰3ç»“æœï¼Œè¿‡æ»¤å·²å…¥åº“çš„ï¼‰
            try:
                web_results = web_search.search(query, k=5)  # å¤šå–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤
                web_filtered = []
                for result in web_results:
                    result_id = get_result_id(result)
                    if result_id not in existing_ids:
                        web_filtered.append(result)
                        existing_ids.add(result_id)
                    if len(web_filtered) >= 3:  # æ¯ä¸ªæŸ¥è¯¢å–3ä¸ªç»“æœ
                        break
                all_initial_results.extend(web_filtered)
                if web_filtered:
                    print(f"  æŸ¥è¯¢ '{query}': è”ç½‘æ‰¾åˆ° {len(web_filtered)} æ¡ç»“æœ")
            except Exception as e:
                print(f"  æŸ¥è¯¢ '{query}': è”ç½‘æ£€ç´¢å¤±è´¥: {e}")
        
        elapsed = time.time() - step_start_time
        print(f"âœ… [æ­¥éª¤4å®Œæˆ] å¹¶è¡Œæ£€ç´¢å®Œæˆ: å…± {len(all_initial_results)} æ¡ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
        
        # å‡†å¤‡ç²¾ç®€çš„ç»“æœæ•°æ®å‘é€åˆ°å‰ç«¯
        results_preview = []
        for result in all_initial_results:
            preview = {
                "source": result.get("source", ""),
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "filename": result.get("filename", ""),  # æ·»åŠ filenameå­—æ®µï¼ˆçŸ¥è¯†åº“ç»“æœéœ€è¦ï¼‰
                "snippet": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
            }
            results_preview.append(preview)
        
        report_progress(4, 6, f"âœ… å¹¶è¡Œæ£€ç´¢å®Œæˆï¼Œå…± {len(all_initial_results)} æ¡ç»“æœ", {
            "retrieval_results": results_preview
        })
        
        # æ­¥éª¤5: ç­›é€‰ç»“æœï¼šé€‰å‡ºçº¦60%çš„ç»“æœï¼ˆä¿ç•™æ›´å¤šä¿¡æ¯ï¼‰
        step_start_time = time.time()
        report_progress(5, 6, "ğŸ” ç­›é€‰æ£€ç´¢ç»“æœ...")
        print(f"\nâ±ï¸  [æ­¥éª¤5å¼€å§‹] ç­›é€‰æ£€ç´¢ç»“æœ - {time.strftime('%H:%M:%S')}")
        filtered_results = []
        if all_initial_results:
            outline_markdown = outline.get("outline_markdown", "")
            if not outline_markdown:
                outline_markdown = _format_outline_markdown(outline)
            
            # ç¡®ä¿ outline_markdown ä¸ä¸ºç©ºï¼ˆé¿å… LLM è¾“å…¥é•¿åº¦é”™è¯¯ï¼‰
            if not outline_markdown or len(outline_markdown.strip()) == 0:
                print(f"âš ï¸  è­¦å‘Š: outline_markdown ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
                outline_markdown = f"# {outline.get('title', 'æŠ¥å‘Š')}\n\n## {current_section.get('level1_title', 'ç« èŠ‚')}\n\n### {current_section.get('level2_title', 'å†…å®¹')}"
            
            # è®¡ç®—ç›®æ ‡ç­›é€‰æ•°é‡ï¼šä¿ç•™çº¦60%çš„ç»“æœï¼Œä½†è‡³å°‘ä¿ç•™3ä¸ªï¼Œæœ€å¤šä¿ç•™10ä¸ª
            target_count = max(3, min(10, int(len(all_initial_results) * 0.6)))
            filtered_results = filter_agent.filter_results(
                search_results=all_initial_results,
                section=current_section,
                search_queries=search_queries,
                outline=outline_markdown,
                target_count=target_count
            )
            
            # æ­¥éª¤6: å°†ç­›é€‰ç»“æœåŠ å…¥ä¸´æ—¶çŸ¥è¯†åº“
            if filtered_results:
                elapsed = time.time() - step_start_time
                print(f"âœ… [æ­¥éª¤5å®Œæˆ] ç­›é€‰å®Œæˆ: ä» {len(all_initial_results)} æ¡ä¸­é€‰å‡º {len(filtered_results)} æ¡ - è€—æ—¶ {elapsed:.2f}ç§’")
                
                # å‡†å¤‡ç²¾ç®€çš„ç­›é€‰ç»“æœæ•°æ®å‘é€åˆ°å‰ç«¯
                filtered_preview = []
                for result in filtered_results:
                    preview = {
                        "source": result.get("source", ""),
                        "title": result.get("title", ""),
                        "url": result.get("url", ""),
                        "filename": result.get("filename", ""),  # æ·»åŠ filenameå­—æ®µï¼ˆçŸ¥è¯†åº“ç»“æœéœ€è¦ï¼‰
                        "snippet": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
                    }
                    filtered_preview.append(preview)
                
                report_progress(5, 6, f"âœ… å·²ç­›é€‰å‡º {len(filtered_results)} æ¡é«˜è´¨é‡ç»“æœï¼ˆä» {len(all_initial_results)} æ¡ä¸­ï¼‰", {
                    "filtered_results": filtered_preview
                })
                
                step_start_time = time.time()
                report_progress(6, 6, "ğŸ” ä¿å­˜åˆ°ä¸´æ—¶çŸ¥è¯†åº“...")
                print(f"\nâ±ï¸  [æ­¥éª¤6å¼€å§‹] ä¿å­˜åˆ°ä¸´æ—¶çŸ¥è¯†åº“ - {time.strftime('%H:%M:%S')}")
                temp_kb.add_search_results(filtered_results)
                elapsed = time.time() - step_start_time
                print(f"âœ… [æ­¥éª¤6å®Œæˆ] å·²å…¥åº“ {len(filtered_results)} æ¡ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
                report_progress(6, 6, f"âœ… å·²ä¿å­˜ {len(filtered_results)} æ¡ç»“æœåˆ°ä¸´æ—¶åº“")
        
        # åˆå¹¶ç»“æœï¼šä¸´æ—¶KBç»“æœ + ç­›é€‰åçš„ç»“æœ
        initial_temp_kb_results = temp_kb_results + filtered_results
        state["initial_temp_kb_results"] = initial_temp_kb_results
        print(f"\nâœ… prepare_section èŠ‚ç‚¹å®Œæˆ: ä¸´æ—¶KB {len(temp_kb_results)} æ¡ + ç­›é€‰å {len(filtered_results)} æ¡ = å…± {len(initial_temp_kb_results)} æ¡ç»“æœ")
    
    return state


def collect_info_node(state: WorkflowState) -> WorkflowState:
    """ä¿¡æ¯æ”¶é›†èŠ‚ç‚¹ - æ–°æµç¨‹ï¼šå…ˆåˆ¤æ–­å……è¶³æ€§ï¼Œä¸è¶³åˆ™æ£€ç´¢"""
    import hashlib
    import time
    
    current_section = state.get("current_section")
    writing_agent = state.get("writing_agent")
    temp_kb = state.get("temp_kb")
    history_manager = state.get("history_manager")
    outline = state.get("outline", {})
    requirement = state.get("requirement", "")
    
    if not current_section:
        raise WorkflowError("å½“å‰ç« èŠ‚ä¿¡æ¯ä¸ºç©º")
    if not writing_agent:
        raise WorkflowError("å†™ä½œæ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
    
    print("\n" + "-" * 50)
    print("ä¿¡æ¯æ”¶é›†é˜¶æ®µ")
    print("-" * 50)
    
    # ä½¿ç”¨å…¨å±€è¿›åº¦ç®¡ç†å™¨
    from ..tools.progress_manager import get_progress_manager
    progress_manager = get_progress_manager()
    task_id = state.get("task_id")
    
    def report_progress(step: int, total: int, message: str, data: dict = None):
        """æŠ¥å‘Šæ­¥éª¤è¿›åº¦åˆ°å‰ç«¯"""
        if task_id:
            event_data = {
                "type": "step_progress",
                "node": "collect_info",
                "step": step,
                "total": total,
                "message": message,
                "timestamp": int(time.time() * 1000)
            }
            if data:
                event_data["data"] = data
            progress_manager.report_progress(task_id, event_data)
    
    # è·å–åˆå§‹ä¿¡æ¯
    initial_temp_kb_results = state.get("initial_temp_kb_results", [])
    history_section_ids = state.get("history_section_ids", [])
    
    # æ­¥éª¤1: è·å–å†å²ç« èŠ‚å†…å®¹
    step_start_time = time.time()
    print(f"\nâ±ï¸  [æ­¥éª¤1å¼€å§‹] è·å–å†å²ç« èŠ‚å†…å®¹ - {time.strftime('%H:%M:%S')}")
    history_contents = []
    if history_manager and history_section_ids:
        for section_id in history_section_ids:
            content = history_manager.search_by_title(section_id)
            if content:
                history_contents.append(content)
    elapsed = time.time() - step_start_time
    print(f"âœ… [æ­¥éª¤1å®Œæˆ] è·å– {len(history_contents)} ä¸ªå†å²ç« èŠ‚ - è€—æ—¶ {elapsed:.2f}ç§’")
    
    # æ­¥éª¤2: ç”Ÿæˆå¤§çº²çš„Markdownæ ¼å¼
    step_start_time = time.time()
    print(f"\nâ±ï¸  [æ­¥éª¤2å¼€å§‹] ç”Ÿæˆå¤§çº²Markdown - {time.strftime('%H:%M:%S')}")
    outline_markdown = outline.get("outline_markdown", "")
    if not outline_markdown:
        outline_markdown = _format_outline_markdown(outline)
    
    # ç¡®ä¿ outline_markdown ä¸ä¸ºç©ºï¼ˆé¿å… LLM è¾“å…¥é•¿åº¦é”™è¯¯ï¼‰
    if not outline_markdown or len(outline_markdown.strip()) == 0:
        print(f"âš ï¸  è­¦å‘Š: outline_markdown ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
        outline_markdown = f"# {outline.get('title', 'æŠ¥å‘Š')}\n\n## {current_section.get('level1_title', 'ç« èŠ‚')}\n\n### {current_section.get('level2_title', 'å†…å®¹')}"
    elapsed = time.time() - step_start_time
    print(f"âœ… [æ­¥éª¤2å®Œæˆ] å¤§çº²Markdownå·²å‡†å¤‡ - è€—æ—¶ {elapsed:.2f}ç§’")
    
    # æ­¥éª¤3: ç¬¬ä¸€æ¬¡åˆ¤æ–­ï¼šä¿¡æ¯æ˜¯å¦å……è¶³
    step_start_time = time.time()
    report_progress(1, 4, "ğŸ” è¯„ä¼°ä¿¡æ¯å……è¶³æ€§...")
    print(f"\nâ±ï¸  [æ­¥éª¤3å¼€å§‹] ç¬¬ä¸€æ¬¡ä¿¡æ¯å……è¶³æ€§åˆ¤æ–­ - {time.strftime('%H:%M:%S')}")
    evaluation = writing_agent.evaluate_info_sufficiency(
        section=current_section,
        search_results=initial_temp_kb_results,
        history_sections=history_contents,
        outline=outline_markdown
    )
    state["info_sufficiency_evaluation"] = evaluation
    elapsed = time.time() - step_start_time
    
    if evaluation["sufficient"]:
        # ä¿¡æ¯å……è¶³ï¼Œç›´æ¥ä½¿ç”¨åˆå§‹ç»“æœ
        print(f"âœ… [æ­¥éª¤3å®Œæˆ] ä¿¡æ¯å……è¶³ï¼Œä½¿ç”¨åˆå§‹æ£€ç´¢ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
        report_progress(1, 4, "âœ… ä¿¡æ¯å……è¶³ï¼Œæ— éœ€è¡¥å……æ£€ç´¢")
        state["search_results"] = initial_temp_kb_results
        print(f"\nâœ… collect_info èŠ‚ç‚¹å®Œæˆ: ä¿¡æ¯å……è¶³ï¼Œå…± {len(initial_temp_kb_results)} æ¡ç»“æœ")
        return state
    
    # ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦é¢å¤–æ£€ç´¢
    print(f"âš ï¸  [æ­¥éª¤3å®Œæˆ] ä¿¡æ¯ä¸è¶³ï¼Œç¼ºå¤±ç‚¹: {evaluation['missing_points']} - è€—æ—¶ {elapsed:.2f}ç§’")
    report_progress(1, 4, f"âš ï¸ ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦è¡¥å……æ£€ç´¢", {
        "is_additional_retrieval": True
    })
    
    # æ­¥éª¤4: ç”Ÿæˆé¢å¤–æ£€ç´¢è¯­å¥ï¼ˆåŸºäºç¼ºå¤±ç‚¹ï¼‰
    missing_points = evaluation.get("missing_points", [])
    if not missing_points:
        missing_points = ["éœ€è¦æ›´å¤šç›¸å…³ä¿¡æ¯"]
    
    # æ ¹æ®ç¼ºå¤±ç‚¹æ•°é‡åŠ¨æ€ç¡®å®šæ£€ç´¢è¯­å¥æ•°é‡
    # ç­–ç•¥ï¼šæ ¹æ®ç¼ºå¤±ç‚¹æ•°é‡ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æŸ¥è¯¢æ¥è¦†ç›–æ‰€æœ‰ç¼ºå¤±ç‚¹
    num_missing_points = len(missing_points)
    if num_missing_points <= 2:
        num_queries = 1
    elif num_missing_points <= 4:
        num_queries = 2
    else:
        num_queries = 3  # æœ€å¤š3ä¸ªï¼Œå› ä¸ºä¸€ä¸ªæŸ¥è¯¢å¯ä»¥è¦†ç›–å¤šä¸ªç›¸å…³ç¼ºå¤±ç‚¹
    
    # ç”Ÿæˆæ£€ç´¢è¯­å¥ï¼ˆä½¿ç”¨ç¼ºå¤±ç‚¹ï¼‰
    step_start_time = time.time()
    print(f"\nâ±ï¸  [æ­¥éª¤4å¼€å§‹] ç”Ÿæˆé¢å¤–æ£€ç´¢è¯­å¥ï¼ˆåŸºäºç¼ºå¤±ç‚¹ï¼Œéœ€è¦ {num_queries} ä¸ªï¼‰- {time.strftime('%H:%M:%S')}")
    additional_queries = writing_agent.generate_search_queries_for_missing_points(
        section=current_section,
        missing_points=missing_points,
        num_queries=num_queries,
        requirement=requirement
    )
    state["additional_search_queries"] = additional_queries
    elapsed = time.time() - step_start_time
    print(f"âœ… [æ­¥éª¤4å®Œæˆ] ç”Ÿæˆ {len(additional_queries)} ä¸ªé¢å¤–æ£€ç´¢è¯­å¥ - è€—æ—¶ {elapsed:.2f}ç§’")
    report_progress(1, 4, f"å·²ç”Ÿæˆ {len(additional_queries)} ä¸ªè¡¥å……æ£€ç´¢æŸ¥è¯¢", {
        "additional_search_queries": additional_queries
    })
    
    # æ­¥éª¤5: å¹¶è¡Œæ£€ç´¢ï¼šçŸ¥è¯†åº“ + è”ç½‘
    from ..services.knowledge_base import get_knowledge_base_manager
    from ..services.web_search import get_web_search_manager
    
    main_kb = get_knowledge_base_manager()
    web_search = get_web_search_manager()
    
    # è·å–å·²å…¥åº“çš„ç»“æœIDï¼ˆç”¨äºå»é‡ï¼‰
    # ä½¿ç”¨ content+source çš„hashä½œä¸ºID
    def get_result_id(result):
        content = result.get("content", "")
        source = result.get("source", "")
        id_str = f"{content}|{source}"
        return hashlib.md5(id_str.encode()).hexdigest()
    
    existing_ids = set()
    for result in initial_temp_kb_results:
        existing_ids.add(get_result_id(result))
    
    all_additional_results = []
    
    step_start_time = time.time()
    report_progress(2, 4, "ğŸ” è¡¥å……æ£€ç´¢ï¼ˆçŸ¥è¯†åº“ + è”ç½‘ï¼‰...")
    print(f"\nâ±ï¸  [æ­¥éª¤5å¼€å§‹] å¹¶è¡Œæ£€ç´¢ï¼ˆçŸ¥è¯†åº“ + è”ç½‘ï¼‰- {time.strftime('%H:%M:%S')}")
    for query in additional_queries:
        # æ£€ç´¢çŸ¥è¯†åº“ï¼ˆå–å‰3ç»“æœï¼Œè¿‡æ»¤å·²å…¥åº“çš„ï¼‰
        try:
            kb_results = main_kb.search(query, k=5)  # å¤šå–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤
            kb_filtered = []
            for result in kb_results:
                result_id = get_result_id(result)
                if result_id not in existing_ids:
                    kb_filtered.append(result)
                    existing_ids.add(result_id)
                if len(kb_filtered) >= 3:  # æ¯ä¸ªæŸ¥è¯¢å–3ä¸ªç»“æœ
                    break
            all_additional_results.extend(kb_filtered)
            if kb_filtered:
                print(f"  æŸ¥è¯¢ '{query}': çŸ¥è¯†åº“æ‰¾åˆ° {len(kb_filtered)} æ¡ç»“æœ")
        except Exception as e:
            print(f"  æŸ¥è¯¢ '{query}': çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
        
        # è”ç½‘æ£€ç´¢ï¼ˆå–å‰3ç»“æœï¼Œè¿‡æ»¤å·²å…¥åº“çš„ï¼‰
        try:
            web_results = web_search.search(query, k=5)  # å¤šå–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤
            web_filtered = []
            for result in web_results:
                result_id = get_result_id(result)
                if result_id not in existing_ids:
                    web_filtered.append(result)
                    existing_ids.add(result_id)
                if len(web_filtered) >= 3:  # æ¯ä¸ªæŸ¥è¯¢å–3ä¸ªç»“æœ
                    break
            all_additional_results.extend(web_filtered)
            if web_filtered:
                print(f"  æŸ¥è¯¢ '{query}': è”ç½‘æ‰¾åˆ° {len(web_filtered)} æ¡ç»“æœ")
        except Exception as e:
            print(f"  æŸ¥è¯¢ '{query}': è”ç½‘æ£€ç´¢å¤±è´¥: {e}")
    
    state["additional_search_results"] = all_additional_results
    elapsed = time.time() - step_start_time
    print(f"âœ… [æ­¥éª¤5å®Œæˆ] å¹¶è¡Œæ£€ç´¢å®Œæˆ: å…± {len(all_additional_results)} æ¡ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
    
    # å‡†å¤‡ç²¾ç®€çš„é¢å¤–æ£€ç´¢ç»“æœæ•°æ®å‘é€åˆ°å‰ç«¯
    additional_results_preview = []
    for result in all_additional_results:
        preview = {
            "source": result.get("source", ""),
            "title": result.get("title", ""),
            "url": result.get("url", ""),
            "filename": result.get("filename", ""),  # æ·»åŠ filenameå­—æ®µï¼ˆçŸ¥è¯†åº“ç»“æœéœ€è¦ï¼‰
            "snippet": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
        }
        additional_results_preview.append(preview)
    
    report_progress(2, 4, f"âœ… è¡¥å……æ£€ç´¢å®Œæˆï¼Œå…± {len(all_additional_results)} æ¡ç»“æœ", {
        "additional_retrieval_results": additional_results_preview
    })
    
    # æ­¥éª¤6: ç­›é€‰ç»“æœï¼šä½¿ç”¨ç­›é€‰æ™ºèƒ½ä½“é€‰å‡ºçº¦60%çš„ç»“æœï¼ˆä¿ç•™æ›´å¤šä¿¡æ¯ï¼‰
    step_start_time = time.time()
    report_progress(3, 4, "ğŸ” ç­›é€‰è¡¥å……ç»“æœ...")
    print(f"\nâ±ï¸  [æ­¥éª¤6å¼€å§‹] ç­›é€‰é¢å¤–æ£€ç´¢ç»“æœ - {time.strftime('%H:%M:%S')}")
    if all_additional_results:
        from ..agents.result_filter_agent import ResultFilterAgent
        filter_agent = ResultFilterAgent()
        
        # è·å–ç¼ºå¤±ç‚¹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        missing_points = evaluation.get("missing_points", [])
        
        # è®¡ç®—ç›®æ ‡ç­›é€‰æ•°é‡ï¼šä¿ç•™çº¦60%çš„ç»“æœï¼Œä½†è‡³å°‘ä¿ç•™3ä¸ªï¼Œæœ€å¤šä¿ç•™10ä¸ª
        target_count = max(3, min(10, int(len(all_additional_results) * 0.6)))
        filtered_results = filter_agent.filter_results(
            search_results=all_additional_results,
            section=current_section,
            search_queries=additional_queries,
            outline=outline_markdown,
            target_count=target_count,
            missing_points=missing_points if missing_points else None
        )
        state["filtered_results"] = filtered_results
        elapsed = time.time() - step_start_time
        print(f"âœ… [æ­¥éª¤6å®Œæˆ] ç­›é€‰å®Œæˆ: ä» {len(all_additional_results)} æ¡ä¸­é€‰å‡º {len(filtered_results)} æ¡ - è€—æ—¶ {elapsed:.2f}ç§’")
        
        # å‡†å¤‡ç²¾ç®€çš„é¢å¤–ç­›é€‰ç»“æœæ•°æ®å‘é€åˆ°å‰ç«¯
        additional_filtered_preview = []
        for result in filtered_results:
            preview = {
                "source": result.get("source", ""),
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "filename": result.get("filename", ""),  # æ·»åŠ filenameå­—æ®µï¼ˆçŸ¥è¯†åº“ç»“æœéœ€è¦ï¼‰
                "snippet": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")
            }
            additional_filtered_preview.append(preview)
        
        report_progress(3, 4, f"âœ… å·²ç­›é€‰å‡º {len(filtered_results)} æ¡é«˜è´¨é‡ç»“æœï¼ˆä» {len(all_additional_results)} æ¡ä¸­ï¼‰", {
            "additional_filtered_results": additional_filtered_preview
        })
        
        # æ­¥éª¤7: å°†ç­›é€‰ç»“æœåŠ å…¥ä¸´æ—¶çŸ¥è¯†åº“
        if temp_kb and filtered_results:
            step_start_time = time.time()
            print(f"\nâ±ï¸  [æ­¥éª¤7å¼€å§‹] ä¿å­˜åˆ°ä¸´æ—¶çŸ¥è¯†åº“ - {time.strftime('%H:%M:%S')}")
            temp_kb.add_search_results(filtered_results)
            elapsed = time.time() - step_start_time
            print(f"âœ… [æ­¥éª¤7å®Œæˆ] å·²å…¥åº“ {len(filtered_results)} æ¡ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœï¼ˆåˆå§‹ + ç­›é€‰åçš„ï¼‰
        final_results = initial_temp_kb_results + filtered_results
        
        # æ­¥éª¤8: ç¬¬äºŒæ¬¡åˆ¤æ–­ï¼šä¿¡æ¯æ˜¯å¦å……è¶³
        step_start_time = time.time()
        report_progress(4, 4, "ğŸ” æœ€ç»ˆè¯„ä¼°...")
        print(f"\nâ±ï¸  [æ­¥éª¤8å¼€å§‹] ç¬¬äºŒæ¬¡ä¿¡æ¯å……è¶³æ€§åˆ¤æ–­ - {time.strftime('%H:%M:%S')}")
        evaluation2 = writing_agent.evaluate_info_sufficiency(
            section=current_section,
            search_results=final_results,
            history_sections=history_contents,
            outline=outline_markdown
        )
        state["info_sufficiency_evaluation"] = evaluation2
        elapsed = time.time() - step_start_time
        
        if evaluation2["sufficient"]:
            print(f"âœ… [æ­¥éª¤8å®Œæˆ] ä¿¡æ¯å……è¶³ï¼Œä½¿ç”¨åˆå¹¶åçš„æ£€ç´¢ç»“æœ - è€—æ—¶ {elapsed:.2f}ç§’")
            report_progress(4, 4, "âœ… ä¿¡æ¯å……è¶³")
        else:
            print(f"âš ï¸  [æ­¥éª¤8å®Œæˆ] ä¿¡æ¯ä»ç„¶ä¸è¶³ï¼Œå°†ä½¿ç”¨ç°æœ‰ç»“æœå¼ºåˆ¶å†™ä½œ - è€—æ—¶ {elapsed:.2f}ç§’")
            report_progress(4, 4, "âš ï¸ ä¿¡æ¯ä»ä¸è¶³ï¼Œä½¿ç”¨ç°æœ‰ç»“æœç»§ç»­æ’°å†™")
        
        state["search_results"] = final_results
    else:
        # å¦‚æœæ²¡æœ‰é¢å¤–ç»“æœï¼Œä½¿ç”¨åˆå§‹ç»“æœ
        print("âš ï¸  æœªæ£€ç´¢åˆ°é¢å¤–ç»“æœï¼Œä½¿ç”¨åˆå§‹æ£€ç´¢ç»“æœ")
        state["search_results"] = initial_temp_kb_results
    
    print(f"\nâœ… collect_info èŠ‚ç‚¹å®Œæˆ: æœ€ç»ˆæ£€ç´¢ç»“æœæ•° {len(state['search_results'])} æ¡")
    
    return state


def writing_node(state: WorkflowState) -> WorkflowState:
    """å†™ä½œèŠ‚ç‚¹ - æ’°å†™ç« èŠ‚å†…å®¹"""
    import time
    
    current_section = state.get("current_section")
    search_results = state.get("search_results", [])
    history_section_ids = state.get("history_section_ids", [])  # ä½¿ç”¨IDè·å–å†…å®¹
    outline = state.get("outline", {})
    writing_agent = state.get("writing_agent")
    all_written_sections = state.get("all_written_sections", [])
    history_manager = state.get("history_manager")
    
    if not current_section:
        raise WorkflowError("å½“å‰ç« èŠ‚ä¿¡æ¯ä¸ºç©º")
    if not writing_agent:
        raise WorkflowError("å†™ä½œæ™ºèƒ½ä½“æœªåˆå§‹åŒ–")
    
    print("\n" + "-" * 50)
    print("å†™ä½œé˜¶æ®µ")
    print("-" * 50)
    
    node_start_time = time.time()
    
    # ç”Ÿæˆå‰æ–‡æ‘˜è¦ï¼ˆæœ€è¿‘2-3ä¸ªç« èŠ‚çš„æ‘˜è¦ï¼‰
    previous_summary = _generate_previous_summary(all_written_sections[-3:])
    
    # ç”Ÿæˆå¤§çº²çš„Markdownæ ¼å¼
    outline_markdown = outline.get("outline_markdown", "")
    if not outline_markdown:
        outline_markdown = _format_outline_markdown(outline)
    
    # æ ¹æ®IDè·å–å†å²ç« èŠ‚å†…å®¹
    history_contents = []
    if history_manager and history_section_ids:
        for section_id in history_section_ids:
            content = history_manager.search_by_title(section_id)
            if content:
                history_contents.append(content)
    
    # è®¡ç®—æ€»å­—æ•°å’Œæ€»ç« èŠ‚æ•°ï¼Œä¼ é€’ç»™å†™ä½œæ™ºèƒ½ä½“
    total_words = outline.get("estimated_words", 0)
    sections = state.get("sections", [])
    total_sections = len(sections)
    
    # è®¡ç®—å·²å†™å†…å®¹çš„å­—æ•°
    from ..tools.utils import estimate_word_count
    all_written_sections = state.get("all_written_sections", [])
    written_words = 0
    for written_section in all_written_sections:
        content = written_section.get("content", "")
        # ä½¿ç”¨å­—æ•°ç»Ÿè®¡å‡½æ•°ï¼ˆé€‚ç”¨äºä¸­è‹±æ–‡æ··åˆï¼‰
        written_words += estimate_word_count(content)
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°å­—æ•°ä¿¡æ¯
    print(f"\nğŸ“Š [å­—æ•°ç»Ÿè®¡]")
    print(f"  æ€»å­—æ•°è¦æ±‚: {total_words} å­—")
    print(f"  å·²å†™ç« èŠ‚æ•°: {len(all_written_sections)} ä¸ª")
    print(f"  å·²å†™å­—æ•°: {written_words} å­—")
    print(f"  å‰©ä½™å­—æ•°: {total_words - written_words if total_words > 0 else 'ä¸é™åˆ¶'}")
    print(f"  å‰©ä½™ç« èŠ‚æ•°: {total_sections - current_section.get('index', 0) + 1} ä¸ª")
    
    # æ’°å†™ç« èŠ‚
    print(f"\nâ±ï¸  [å¼€å§‹å†™ä½œ] è°ƒç”¨ LLM æ’°å†™ç« èŠ‚ - {time.strftime('%H:%M:%S')}")
    try:
        result = writing_agent.write_section(
            section=current_section,
            search_results=search_results,
            history_sections=history_contents,  # ä¼ é€’å®Œæ•´å†…å®¹
            outline=outline_markdown,
            previous_sections_summary=previous_summary,
            total_words=total_words if total_words > 0 else None,
            total_sections=total_sections if total_sections > 0 else None,
            written_words=written_words if written_words > 0 else None
        )
        
        # write_section ç°åœ¨è¿”å› dict: {"content": str, "citations": List[Dict], "chart_requirement": Optional[Dict]}
        content = result["content"]
        citations = result.get("citations", [])
        chart_requirement = result.get("chart_requirement")
        
        state["written_content"] = content
        state["written_citations"] = citations  # ä¿å­˜å¼•ç”¨ä¿¡æ¯
        state["chart_requirement"] = chart_requirement  # ä¿å­˜å›¾è¡¨éœ€æ±‚
        
        elapsed = time.time() - node_start_time
        print(f"\nâœ… writing èŠ‚ç‚¹å®Œæˆ")
        print(f"  å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"  ä¼°ç®—å­—æ•°: {estimate_word_count(content)} å­—")
        print(f"  å¼•ç”¨æ•°é‡: {len(citations)} ä¸ª")
        if chart_requirement:
            print(f"  ğŸ“Š éœ€è¦å›¾è¡¨: {chart_requirement.get('chart_type', 'unknown')}")
        print(f"  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        
    except Exception as e:
        raise WorkflowError(f"æ’°å†™ç« èŠ‚å¤±è´¥: {str(e)}") from e
    
    return state


def save_section_node(state: WorkflowState) -> WorkflowState:
    """ä¿å­˜ç« èŠ‚èŠ‚ç‚¹ - å°†å®Œæˆçš„ç« èŠ‚å­˜å…¥å†å²"""
    import time
    
    current_section = state.get("current_section")
    written_content = state.get("written_content", "")
    written_citations = state.get("written_citations", [])  # è·å–å¼•ç”¨ä¿¡æ¯
    chart_requirement = state.get("chart_requirement")  # è·å–å›¾è¡¨éœ€æ±‚
    history_manager = state.get("history_manager")
    
    if not current_section:
        raise WorkflowError("å½“å‰ç« èŠ‚ä¿¡æ¯ä¸ºç©º")
    if not written_content:
        raise WorkflowError("ç« èŠ‚å†…å®¹ä¸ºç©º")
    if not history_manager:
        raise WorkflowError("å†å²ç®¡ç†å™¨æœªåˆå§‹åŒ–")
    
    print("\n" + "-" * 50)
    print("ä¿å­˜ç« èŠ‚")
    print("-" * 50)
    
    node_start_time = time.time()
    print(f"â±ï¸  [å¼€å§‹ä¿å­˜] - {time.strftime('%H:%M:%S')}")
    
    # ä¿å­˜åˆ°å†å²å†™ä½œç®¡ç†å™¨ï¼ˆç°åœ¨æ˜¯ä¸€çº§ç« èŠ‚ï¼‰
    level1_title = current_section.get("level1_title", "")
    level2_titles = current_section.get("level2_titles", [])
    section_id = current_section.get("section_id", "")
    
    # æ„å»ºå®Œæ•´çš„ç« èŠ‚æ ‡é¢˜ï¼ˆåŒ…å«Markdownæ ‡è®°ï¼‰- ç°åœ¨æ˜¯ä¸€çº§æ ‡é¢˜
    full_level1_title = f"## {level1_title}"
    
    history_manager.add_section(
        title=full_level1_title,
        content=written_content,
        parent_title=None,  # ä¸€çº§ç« èŠ‚æ²¡æœ‰çˆ¶æ ‡é¢˜
        section_id=section_id
    )
    
    # æ·»åŠ åˆ°å·²å†™ç« èŠ‚åˆ—è¡¨
    all_written_sections = state.get("all_written_sections", [])
    section_data = {
        "section_id": section_id,
        "level1_title": level1_title,
        "level2_titles": level2_titles,  # ä¿å­˜äºŒçº§æ ‡é¢˜åˆ—è¡¨
        "content": written_content,
        "citations": written_citations,  # ä¿å­˜å¼•ç”¨ä¿¡æ¯
        "chart_requirement": chart_requirement,  # ä¿å­˜å›¾è¡¨éœ€æ±‚
        "chart_url": None,  # å›¾è¡¨URLï¼ˆå¼‚æ­¥ç”Ÿæˆåå¡«å……ï¼‰
        "chart_generating": False,  # å›¾è¡¨æ˜¯å¦æ­£åœ¨ç”Ÿæˆ
    }
    all_written_sections.append(section_data)
    state["all_written_sections"] = all_written_sections
    
    # å¦‚æœæœ‰å›¾è¡¨éœ€æ±‚ï¼Œå¼‚æ­¥ç”Ÿæˆå›¾è¡¨
    if chart_requirement:
        import threading
        from ..tools.chart_generator import get_chart_generator
        
        def generate_chart_async():
            """å¼‚æ­¥ç”Ÿæˆå›¾è¡¨"""
            try:
                chart_gen = get_chart_generator()
                print(f"\nğŸ“Š [å¼‚æ­¥] å¼€å§‹ä¸ºç« èŠ‚ '{level1_title}' ç”Ÿæˆå›¾è¡¨...")
                
                chart_url = chart_gen.generate_chart_from_content(
                    section_content=written_content,
                    chart_requirement=chart_requirement,
                    section=current_section
                )
                
                if chart_url:
                    # æ›´æ–°ç« èŠ‚æ•°æ®ä¸­çš„å›¾è¡¨URL
                    section_data["chart_url"] = chart_url
                    print(f"âœ… [å¼‚æ­¥] å›¾è¡¨ç”ŸæˆæˆåŠŸ: {chart_url}")
                else:
                    print(f"âš ï¸  [å¼‚æ­¥] å›¾è¡¨ç”Ÿæˆå¤±è´¥")
            except Exception as e:
                print(f"âš ï¸  [å¼‚æ­¥] å›¾è¡¨ç”Ÿæˆå‡ºé”™: {e}")
            finally:
                section_data["chart_generating"] = False
        
        section_data["chart_generating"] = True
        thread = threading.Thread(target=generate_chart_async, daemon=True)
        thread.start()
        print(f"ğŸ“Š [å¼‚æ­¥] å·²å¯åŠ¨å›¾è¡¨ç”Ÿæˆä»»åŠ¡ï¼ˆç« èŠ‚: {level1_title}ï¼‰")
    
    # æ›´æ–°ç´¢å¼•
    state["current_section_index"] = state.get("current_section_index", 0) + 1
    
    elapsed = time.time() - node_start_time
    print(f"\nâœ… save_section èŠ‚ç‚¹å®Œæˆ")
    print(f"  ç« èŠ‚ID: {section_id}")
    print(f"  æ ‡é¢˜: {level1_title}")
    print(f"  åŒ…å«äºŒçº§æ ‡é¢˜: {len(level2_titles)} ä¸ª")
    print(f"  å¼•ç”¨æ•°: {len(written_citations)} ä¸ª")
    print(f"  æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    
    return state


def _insert_chart_after_section(content: str, section_title: str, chart_markdown: str) -> Optional[str]:
    """
    åœ¨æŒ‡å®šç« èŠ‚æ ‡é¢˜çš„æœ«å°¾æ’å…¥å›¾è¡¨
    
    Args:
        content: ç« èŠ‚å†…å®¹
        section_title: ç« èŠ‚æ ‡é¢˜ï¼ˆ## ä¸€çº§æ ‡é¢˜ æˆ– ### äºŒçº§æ ‡é¢˜ï¼‰
        chart_markdown: å›¾è¡¨çš„ Markdown ä»£ç 
    
    Returns:
        æ’å…¥åçš„å†…å®¹ï¼Œå¦‚æœæ‰¾ä¸åˆ°ç« èŠ‚æ ‡é¢˜åˆ™è¿”å› None
    """
    import re
    
    # è§„èŒƒåŒ–ç« èŠ‚æ ‡é¢˜ï¼ˆç§»é™¤å¯èƒ½çš„ç©ºæ ¼ï¼‰
    section_title = section_title.strip()
    
    # å¦‚æœæ ‡é¢˜ä¸ä»¥ ## æˆ– ### å¼€å¤´ï¼Œå°è¯•æ·»åŠ 
    if not section_title.startswith('#'):
        # å°è¯•åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜æ–‡æœ¬è€Œä¸æ˜¯å®Œæ•´çš„ Markdown æ ¼å¼
        # å…ˆå°è¯•ä½œä¸ºäºŒçº§æ ‡é¢˜
        if not section_title.startswith('###'):
            section_title_with_marker = f"### {section_title}"
        else:
            section_title_with_marker = section_title
    else:
        section_title_with_marker = section_title
    
    # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜åœ¨å†…å®¹ä¸­çš„ä½ç½®
    # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š## æ ‡é¢˜ æˆ– ## æ ‡é¢˜\n
    patterns = [
        re.escape(section_title_with_marker) + r'\s*\n',  # æ ‡é¢˜åè·Ÿæ¢è¡Œ
        re.escape(section_title_with_marker) + r'(?=\n|$)',  # æ ‡é¢˜åè·Ÿæ¢è¡Œæˆ–ç»“å°¾
    ]
    
    section_start = -1
    for pattern in patterns:
        match = re.search(pattern, content)
        if match:
            section_start = match.end()
            break
    
    if section_start == -1:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                section_start = match.end()
                break
    
    if section_start == -1:
        return None
    
    # ç¡®å®šç« èŠ‚çš„ç»“æŸä½ç½®
    # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„æ ‡é¢˜
    remaining_content = content[section_start:]
    
    # åˆ¤æ–­å½“å‰æ ‡é¢˜çš„çº§åˆ«
    if section_title_with_marker.startswith('###'):
        # äºŒçº§æ ‡é¢˜ï¼šæŸ¥æ‰¾ä¸‹ä¸€ä¸ª ### æˆ– ##
        next_section_pattern = r'\n(?:###|##)\s+'
    elif section_title_with_marker.startswith('##'):
        # ä¸€çº§æ ‡é¢˜ï¼šæŸ¥æ‰¾ä¸‹ä¸€ä¸ª ##
        next_section_pattern = r'\n##\s+'
    else:
        # é»˜è®¤å½“ä½œäºŒçº§æ ‡é¢˜å¤„ç†
        next_section_pattern = r'\n(?:###|##)\s+'
    
    next_section_match = re.search(next_section_pattern, remaining_content)
    
    if next_section_match:
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œåœ¨è¯¥ç« èŠ‚ä¹‹å‰æ’å…¥
        section_end = section_start + next_section_match.start()
    else:
        # æ²¡æœ‰ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œæ’å…¥åˆ°å½“å‰ç« èŠ‚å†…å®¹çš„æœ«å°¾
        section_end = len(content)
    
    # åœ¨ç« èŠ‚æœ«å°¾æ’å…¥å›¾è¡¨
    # ç¡®ä¿åœ¨æ’å…¥ä½ç½®ä¹‹å‰æœ‰é€‚å½“çš„æ¢è¡Œ
    insert_position = section_end
    # ç§»é™¤æœ«å°¾çš„ç©ºç™½å­—ç¬¦ï¼Œç„¶åæ·»åŠ å›¾è¡¨
    before_insert = content[:insert_position].rstrip()
    after_insert = content[insert_position:]
    
    # ç»„åˆï¼šåŸå†…å®¹ï¼ˆå»é™¤æœ«å°¾ç©ºç™½ï¼‰+ å›¾è¡¨ + åç»­å†…å®¹
    new_content = before_insert + chart_markdown + after_insert
    
    return new_content


def complete_node(state: WorkflowState) -> WorkflowState:
    """å®ŒæˆèŠ‚ç‚¹ - ç­‰å¾…æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆå¹¶æ’å…¥åˆ°æŠ¥å‘Šä¸­ï¼Œç„¶åæ¸…ç©ºä¸´æ—¶çŸ¥è¯†åº“"""
    import time
    
    print("\n" + "=" * 50)
    print("å·¥ä½œæµå®Œæˆ - ç­‰å¾…å›¾è¡¨ç”Ÿæˆå¹¶ç»„è£…æŠ¥å‘Š")
    print("=" * 50)
    
    all_written_sections = state.get("all_written_sections", [])
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦ç­‰å¾…çš„å›¾è¡¨ç”Ÿæˆä»»åŠ¡
    sections_with_charts = [s for s in all_written_sections if s.get("chart_requirement")]
    
    if sections_with_charts:
        print(f"\nğŸ“Š æ£€æµ‹åˆ° {len(sections_with_charts)} ä¸ªç« èŠ‚éœ€è¦å›¾è¡¨ï¼Œç­‰å¾…å¼‚æ­¥ç”Ÿæˆå®Œæˆ...")
        
        # ç­‰å¾…æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
        max_wait_time = 60
        check_interval = 0.5
        start_wait = time.time()
        
        while time.time() - start_wait < max_wait_time:
            all_done = True
            for section in sections_with_charts:
                if section.get("chart_generating", False):
                    all_done = False
                    break
            
            if all_done:
                break
            
            time.sleep(check_interval)
        
        if time.time() - start_wait >= max_wait_time:
            print(f"âš ï¸  ç­‰å¾…å›¾è¡¨ç”Ÿæˆè¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰ï¼Œç»§ç»­å¤„ç†å·²å®Œæˆçš„å›¾è¡¨")
    
    # æ”¶é›†æ‰€æœ‰å·²ç”Ÿæˆçš„å›¾è¡¨URL
    chart_urls = {}  # section_id -> chart_url
    for section in all_written_sections:
        section_id = section.get("section_id", "")
        chart_url = section.get("chart_url")
        if chart_url:
            chart_urls[section_id] = chart_url
            print(f"âœ… ç« èŠ‚ '{section.get('level1_title', '')}' å›¾è¡¨å·²å°±ç»ª: {chart_url}")
    
    # å°†å›¾è¡¨URLæ’å…¥åˆ°ç« èŠ‚å†…å®¹ä¸­ï¼ˆæ’å…¥åˆ°æŒ‡å®šç« èŠ‚æ ‡é¢˜çš„æœ«å°¾ï¼‰
    if chart_urls:
        print(f"\nğŸ“Š æ’å…¥ {len(chart_urls)} ä¸ªå›¾è¡¨åˆ°æŠ¥å‘Šä¸­...")
        for section in all_written_sections:
            section_id = section.get("section_id", "")
            if section_id in chart_urls:
                chart_url = chart_urls[section_id]
                chart_requirement = section.get("chart_requirement", {})
                chart_description = chart_requirement.get("chart_description", "å›¾è¡¨")
                insert_after_section = chart_requirement.get("insert_after_section", "")
                
                # ç”Ÿæˆå›¾è¡¨çš„ Markdown
                chart_markdown = f"\n\n![{chart_description}]({chart_url})\n\n*å›¾ï¼š{chart_description}*\n\n"
                
                # å°è¯•æ’å…¥åˆ°æŒ‡å®šç« èŠ‚æ ‡é¢˜çš„æœ«å°¾
                content = section["content"]
                if insert_after_section:
                    # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜å¹¶æ’å…¥åˆ°è¯¥ç« èŠ‚æœ«å°¾
                    inserted = _insert_chart_after_section(content, insert_after_section, chart_markdown)
                    if inserted:
                        content = inserted
                        print(f"âœ… å·²æ’å…¥å›¾è¡¨åˆ°ç« èŠ‚ '{section.get('level1_title', '')}' çš„æŒ‡å®šç« èŠ‚ '{insert_after_section}' æœ«å°¾")
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°æŒ‡å®šç« èŠ‚ï¼Œæ’å…¥åˆ°æ•´ä¸ªç« èŠ‚å†…å®¹æœ«å°¾
                        content = content + "\n\n---\n\n### æ•°æ®å¯è§†åŒ–\n\n" + chart_markdown
                        print(f"âš ï¸  æœªæ‰¾åˆ°æŒ‡å®šç« èŠ‚æ ‡é¢˜ '{insert_after_section}'ï¼Œå›¾è¡¨å·²æ’å…¥åˆ°ç« èŠ‚ '{section.get('level1_title', '')}' æœ«å°¾")
                else:
                    # å¦‚æœæœªæŒ‡å®šç« èŠ‚æ ‡é¢˜ï¼Œæ’å…¥åˆ°æ•´ä¸ªç« èŠ‚å†…å®¹æœ«å°¾
                    content = content + "\n\n---\n\n### æ•°æ®å¯è§†åŒ–\n\n" + chart_markdown
                    print(f"âš ï¸  æœªæŒ‡å®šæ’å…¥ç« èŠ‚ï¼Œå›¾è¡¨å·²æ’å…¥åˆ°ç« èŠ‚ '{section.get('level1_title', '')}' æœ«å°¾")
                
                section["content"] = content
    
    # æ¸…ç©ºä¸´æ—¶çŸ¥è¯†åº“
    temp_kb = state.get("temp_kb")
    if temp_kb:
        temp_kb.clear()
        print("\nâœ… ä¸´æ—¶çŸ¥è¯†åº“å·²æ¸…ç©º")
    
    print(f"âœ… å…±å®Œæˆ {len(all_written_sections)} ä¸ªç« èŠ‚")
    if chart_urls:
        print(f"âœ… å…±ç”Ÿæˆ {len(chart_urls)} ä¸ªå›¾è¡¨")
    
    state["is_complete"] = True
    
    return state


def has_more_sections(state: WorkflowState) -> str:
    """åˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šç« èŠ‚"""
    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    
    if current_index >= len(sections):
        return "no"
    return "yes"


def _generate_previous_summary(sections: List[Dict[str, Any]]) -> Optional[str]:
    """ç”Ÿæˆå‰æ–‡æ‘˜è¦"""
    if not sections:
        return None
    
    summaries = []
    for section in sections:
        level2_title = section.get("level2_title", "")
        content = section.get("content", "")
        # å–å†…å®¹çš„å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = content[:200] + "..." if len(content) > 200 else content
        summaries.append(f"### {level2_title}\n{summary}")
    
    return "\n\n".join(summaries)


def _format_outline_markdown(outline: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å¤§çº²ä¸ºMarkdown"""
    if not outline:
        return ""
    
    lines = [f"# {outline.get('title', '')}", ""]
    
    for section in outline.get("sections", []):
        level1_title = section.get("level1_title", "")
        lines.append(f"## {level1_title}")
        lines.append("")
        
        for level2_title in section.get("level2_titles", []):
            lines.append(f"### {level2_title}")
        
        lines.append("")
    
    return "\n".join(lines)


# å·¥ä½œæµå®ä¾‹ï¼ˆå•ä¾‹ï¼‰
_workflow_instance = None


def get_deep_research_workflow():
    """è·å– Deep Research å·¥ä½œæµå®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = create_deep_research_workflow()
    return _workflow_instance

